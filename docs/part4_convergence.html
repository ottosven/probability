<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>4&nbsp; Stochastic Convergence – Probability Theory for Econometricians</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./part3_covariance.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script><script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<meta property="og:title" content="4&nbsp; Stochastic Convergence – Probability Theory for Econometricians">
<meta property="og:description" content="">
<meta property="og:site_name" content="Probability Theory for Econometricians">
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part4_convergence.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Stochastic Convergence</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Probability Theory for Econometricians</a> 
        <div class="sidebar-tools-main">
    <a href="./Probability-Theory-for-Econometricians.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./part1_distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability Distribution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./part2_expectation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Expected Value</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./part3_covariance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple Random Variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./part4_convergence.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Stochastic Convergence</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#estimation" id="toc-estimation" class="nav-link active" data-scroll-target="#estimation"><span class="header-section-number">4.1</span> Estimation</a></li>
  <li><a href="#convergence-in-probability" id="toc-convergence-in-probability" class="nav-link" data-scroll-target="#convergence-in-probability"><span class="header-section-number">4.2</span> Convergence in Probability</a></li>
  <li><a href="#consistency-for-a-parameter" id="toc-consistency-for-a-parameter" class="nav-link" data-scroll-target="#consistency-for-a-parameter"><span class="header-section-number">4.3</span> Consistency for a Parameter</a></li>
  <li><a href="#sufficient-condition-for-consistency" id="toc-sufficient-condition-for-consistency" class="nav-link" data-scroll-target="#sufficient-condition-for-consistency"><span class="header-section-number">4.4</span> Sufficient Condition for Consistency</a></li>
  <li><a href="#mse-decomposition" id="toc-mse-decomposition" class="nav-link" data-scroll-target="#mse-decomposition"><span class="header-section-number">4.5</span> MSE Decomposition</a></li>
  <li><a href="#law-of-large-numbers" id="toc-law-of-large-numbers" class="nav-link" data-scroll-target="#law-of-large-numbers"><span class="header-section-number">4.6</span> Law of Large Numbers</a></li>
  <li><a href="#rate-of-convergence" id="toc-rate-of-convergence" class="nav-link" data-scroll-target="#rate-of-convergence"><span class="header-section-number">4.7</span> Rate of Convergence</a></li>
  <li><a href="#convergence-in-distribution" id="toc-convergence-in-distribution" class="nav-link" data-scroll-target="#convergence-in-distribution"><span class="header-section-number">4.8</span> Convergence in Distribution</a></li>
  <li><a href="#asymptotic-distribution-of-an-estimator" id="toc-asymptotic-distribution-of-an-estimator" class="nav-link" data-scroll-target="#asymptotic-distribution-of-an-estimator"><span class="header-section-number">4.9</span> Asymptotic Distribution of an Estimator</a></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem" class="nav-link" data-scroll-target="#central-limit-theorem"><span class="header-section-number">4.10</span> Central Limit Theorem</a></li>
  <li>
<a href="#the-normal-distribution" id="toc-the-normal-distribution" class="nav-link" data-scroll-target="#the-normal-distribution"><span class="header-section-number">4.11</span> The Normal Distribution</a>
  <ul class="collapse">
<li><a href="#multivariate-normal-distribution" id="toc-multivariate-normal-distribution" class="nav-link" data-scroll-target="#multivariate-normal-distribution"><span class="header-section-number">4.11.1</span> Multivariate Normal Distribution</a></li>
  <li><a href="#chi-squared-distribution" id="toc-chi-squared-distribution" class="nav-link" data-scroll-target="#chi-squared-distribution"><span class="header-section-number">4.11.2</span> Chi-squared Distribution</a></li>
  <li><a href="#students-t-distribution" id="toc-students-t-distribution" class="nav-link" data-scroll-target="#students-t-distribution"><span class="header-section-number">4.11.3</span> Student’s t-Distribution</a></li>
  <li><a href="#f-distribution" id="toc-f-distribution" class="nav-link" data-scroll-target="#f-distribution"><span class="header-section-number">4.11.4</span> F-Distribution</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title">
<span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Stochastic Convergence</span>
</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p>Building on the concepts of the previous sections, we now turn to stochastic convergence which helps us understand the behavior of estimators as sample sizes increase. Stochastic convergence provides the theoretical framework for understanding if and how our estimates approach the true population parameters, which is essential for conducting valid statistical inference in econometric analysis.</p>
<section id="estimation" class="level2" data-number="4.1"><h2 data-number="4.1" class="anchored" data-anchor-id="estimation">
<span class="header-section-number">4.1</span> Estimation</h2>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>Parameter and Estimator</strong></p>
<p>A <strong>parameter</strong> <span class="math inline">\theta</span> is a characteristic or feature of a population distribution. Parameters are typically fixed but unknown quantities that we aim to learn about through sampling and estimation.</p>
<p>An <strong>estimator</strong> <span class="math inline">\widehat{\theta}</span> is a function of sample data intended to approximate the unknown parameter <span class="math inline">\theta</span>. Since an estimator is a function of random variables (the sample), it is itself a random variable. When we actually compute the estimator from a specific realized sample, we call the resulting value an <strong>estimate</strong>.</p>
</div>
<p><br></p>
<p>Examples of parameters include:</p>
<ul>
<li>The mean (expected value) <span class="math inline">\mu</span> of a population distribution</li>
<li>The variance <span class="math inline">\sigma^2</span> of a population distribution</li>
<li>The coefficients <span class="math inline">\boldsymbol{\beta}</span> in a regression model</li>
<li>The correlation <span class="math inline">\rho</span> between two random variables</li>
</ul>
<p>For example, the sample mean <span class="math inline">\overline{Y} = \frac{1}{n}\sum_{i=1}^n Y_i</span> is an estimator for the population mean <span class="math inline">\mu = E[Y]</span>.</p>
<p>When we consider the properties of estimators, we often examine what happens as the sample size increases. This leads us to study sequences of random variables.</p>
<p>A <strong>sequence of random variables</strong> <span class="math inline">\{W_n\}_{n=1}^{\infty}</span> is an ordered collection of random variables indexed by sample size <span class="math inline">n</span>. For estimators, we are interested in how the sequence <span class="math inline">\{\widehat{\theta}_n\}_{n=1}^{\infty}</span> behaves as <span class="math inline">n</span> increases, where <span class="math inline">\widehat{\theta}_n</span> represents the estimator based on a sample of size <span class="math inline">n</span>.</p>
<p>The behavior of such sequences as <span class="math inline">n \to \infty</span> is the focus of asymptotic theory in econometrics. Understanding this behavior allows us to evaluate the properties of estimators in large samples, even when their exact finite-sample distributions are intractable.</p>
</section><section id="convergence-in-probability" class="level2" data-number="4.2"><h2 data-number="4.2" class="anchored" data-anchor-id="convergence-in-probability">
<span class="header-section-number">4.2</span> Convergence in Probability</h2>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>Convergence in Probability</strong></p>
<p>A sequence of random variables <span class="math inline">\{W_n\}_{n=1}^{\infty}</span> <strong>converges in probability</strong> to a constant <span class="math inline">c</span> if, for any <span class="math inline">\epsilon &gt; 0</span>, <span class="math display">\lim_{n \to \infty} P(|W_n - c| &gt; \epsilon) = 0</span></p>
<p>Equivalently, this can be expressed as: <span class="math display">\lim_{n \to \infty} P(|W_n - c| \leq \epsilon) = 1</span></p>
<p>This is denoted as <span class="math inline">W_n \overset{p}{\to} c</span>.</p>
</div>
<p><br></p>
<p>Intuitively, convergence in probability means that as the sample size <span class="math inline">n</span> increases, the probability that <span class="math inline">W_n</span> deviates from <span class="math inline">c</span> by more than any fixed positive amount <span class="math inline">\epsilon</span> becomes arbitrarily small.</p>
<p>For example, if <span class="math inline">W_n \overset{p}{\to} c</span>, then for any small <span class="math inline">\epsilon &gt; 0</span> (say, <span class="math inline">\epsilon = 0.01</span>), we can make <span class="math inline">P(|W_n - c| &gt; 0.01)</span> as small as we want by choosing a sufficiently large sample size <span class="math inline">n</span>. This doesn’t mean that <span class="math inline">W_n</span> will exactly equal <span class="math inline">c</span> for large <span class="math inline">n</span>, but rather that the probability of <span class="math inline">W_n</span> being close to <span class="math inline">c</span> approaches 1 as <span class="math inline">n</span> grows.</p>
</section><section id="consistency-for-a-parameter" class="level2" data-number="4.3"><h2 data-number="4.3" class="anchored" data-anchor-id="consistency-for-a-parameter">
<span class="header-section-number">4.3</span> Consistency for a Parameter</h2>
<p>Applying the concept of convergence in probability to estimators leads to the important property of <strong>consistency</strong>.</p>
<p>Good estimators get closer and closer to the true parameter being estimated as the sample size <span class="math inline">n</span> increases, eventually converging to the true parameter value in a hypothetically infinitely large sample.</p>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>Consistency</strong></p>
<p>An estimator <span class="math inline">\widehat{\theta}_n</span> is <strong>consistent</strong> for the parameter <span class="math inline">\theta</span> if: <span class="math display">\widehat{\theta}_n \overset{p}{\to} \theta \quad \text{as} \quad n \to \infty</span></p>
<p>That is, if for any <span class="math inline">\epsilon &gt; 0</span>: <span class="math display">\lim_{n \to \infty} P(|\widehat{\theta}_n - \theta| &gt; \epsilon) = 0</span></p>
</div>
<p><br></p>
<p>Consistency is a minimal requirement for a good estimator. It ensures that with a large enough sample, the estimator will be arbitrarily close to the true parameter with high probability.</p>
<p>If an estimator <span class="math inline">\widehat{\theta}</span> is a continuous random variable, it will almost never equal exactly the true parameter value because for continuous distributions, point probabilities are zero: <span class="math inline">P(\widehat{\theta} = \theta) = 0</span>.</p>
<p>However, the larger the sample size, the higher the probability that <span class="math inline">\widehat{\theta}</span> falls within a small neighborhood around the true value <span class="math inline">\theta</span>. Consistency means that, if we fix some small precision value <span class="math inline">\epsilon &gt; 0</span>, then, <span class="math display">P(|\widehat{\theta} - \theta| \leq \epsilon) = P(\theta - \epsilon \leq \widehat{\theta} \leq \theta + \epsilon)</span> should increase as the sample size <span class="math inline">n</span> grows, approaching 1 in the limit.</p>
<p>This property aligns with our intuition that more data should lead to better estimates, and is fundamental to establishing the reliability of statistical procedures in large samples.</p>
</section><section id="sufficient-condition-for-consistency" class="level2" data-number="4.4"><h2 data-number="4.4" class="anchored" data-anchor-id="sufficient-condition-for-consistency">
<span class="header-section-number">4.4</span> Sufficient Condition for Consistency</h2>
<p>A powerful approach to establishing consistency relies on examining the mean squared error (MSE) of an estimator and applying Markov’s inequality.</p>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>Markov’s Inequality</strong></p>
<p>For any non-negative random variable <span class="math inline">X</span> and any positive constant <span class="math inline">a</span>: <span class="math display">P(X \geq a) \leq \frac{E[X]}{a}</span></p>
</div>
<p><br></p>
<p>Markov’s inequality provides an upper bound on the probability that a non-negative random variable exceeds any positive threshold. While this bound may not be tight, it is extremely useful for proving convergence results.</p>
<p>To apply Markov’s inequality to establish consistency, we consider the squared deviation between the estimator and the parameter: <span class="math display">\begin{align*}
P(|\widehat{\theta}_n - \theta| &gt; \epsilon) &amp;= P((\widehat{\theta}_n - \theta)^2 &gt; \epsilon^2) \\
&amp;\leq \frac{E[(\widehat{\theta}_n - \theta)^2]}{\epsilon^2} \\
&amp;= \frac{MSE(\widehat{\theta}_n)}{\epsilon^2}
\end{align*}</span></p>
<p>where <span class="math inline">MSE(\widehat{\theta}_n) = E[(\widehat{\theta}_n - \theta)^2]</span> is the mean squared error of <span class="math inline">\widehat{\theta}_n</span>.</p>
<p>Therefore, if <span class="math inline">\lim_{n \to \infty} MSE(\widehat{\theta}_n) = 0</span>, then necessarily <span class="math inline">\lim_{n \to \infty} P(|\widehat{\theta}_n - \theta| &gt; \epsilon) = 0</span> for any <span class="math inline">\epsilon &gt; 0</span>.</p>
<p>This leads directly to a sufficient condition for consistency:</p>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>Sufficient Condition for Consistency</strong></p>
<p>Let <span class="math inline">\widehat{\theta}_n</span> be an estimator for parameter <span class="math inline">\theta</span>. If: <span class="math display">\lim_{n \to \infty} MSE(\widehat{\theta}_n) = \lim_{n \to \infty} E[(\widehat{\theta}_n - \theta)^2] = 0</span></p>
<p>Then <span class="math inline">\widehat{\theta}_n</span> is consistent for <span class="math inline">\theta</span>.</p>
</div>
<p><br></p>
<p>This result is particularly valuable because it connects consistency to the MSE, which we can often calculate or bound more easily than working directly with probabilities.</p>
</section><section id="mse-decomposition" class="level2" data-number="4.5"><h2 data-number="4.5" class="anchored" data-anchor-id="mse-decomposition">
<span class="header-section-number">4.5</span> MSE Decomposition</h2>
<p>To analyze when the sufficient condition for consistency holds, we need to understand the components of the MSE. The <strong>bias</strong> of an estimator is defined as:</p>
<p><span class="math display">Bias(\widehat{\theta}_n) = E[\widehat{\theta}_n] - \theta</span></p>
<p>The bias measures the systematic deviation of the estimator from the true parameter. An estimator is <strong>unbiased</strong> if <span class="math inline">Bias(\widehat{\theta}_n) = 0</span> for all sample sizes, and <strong>asymptotically unbiased</strong> if <span class="math inline">\lim_{n \to \infty} Bias(\widehat{\theta}_n) = 0</span>.</p>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>MSE Decomposition</strong></p>
<p><span class="math display">
MSE(\widehat{\theta}_n) = Var(\widehat{\theta}_n) + [Bias(\widehat{\theta}_n)]^2
</span></p>
</div>
<p><br></p>
<p>To derive this decomposition, we add and subtract <span class="math inline">E[\widehat{\theta}_n]</span> inside the squared term:</p>
<p><span class="math display">\begin{align*}
MSE(\widehat{\theta}_n) &amp;= E[(\widehat{\theta}_n - \theta)^2] \\
&amp;= E[(\widehat{\theta}_n - E[\widehat{\theta}_n] + E[\widehat{\theta}_n] - \theta)^2]
\end{align*}</span></p>
<p>Expanding the square, we get:</p>
<p><span class="math display">\begin{align*}
E[(\widehat{\theta}_n - E[\widehat{\theta}_n])^2 + 2(\widehat{\theta}_n - E[\widehat{\theta}_n])(E[\widehat{\theta}_n] - \theta) + (E[\widehat{\theta}_n] - \theta)^2]
\end{align*}</span></p>
<p>The first term is the variance of <span class="math inline">\widehat{\theta}_n</span>. The third term is the squared bias. The middle term simplifies to zero because <span class="math inline">E[\widehat{\theta}_n - E[\widehat{\theta}_n]] = 0</span>.</p>
<p>This decomposition shows that the estimation error comes from two sources: the variability of the estimator around its expected value (variance) and the systematic deviation of the expected value from the true parameter (bias).</p>
<p>This leads to a practical sufficient condition for consistency:</p>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>Practical Sufficient Condition for Consistency</strong></p>
<p>An estimator <span class="math inline">\widehat{\theta}_n</span> is consistent for <span class="math inline">\theta</span> if both of the following conditions hold as <span class="math inline">n \to \infty</span>:</p>
<ol type="1">
<li>
<span class="math inline">Bias(\widehat{\theta}_n) \to 0</span> (asymptotically unbiased)</li>
<li>
<span class="math inline">Var(\widehat{\theta}_n) \to 0</span> (variance approaches zero)</li>
</ol>
</div>
<p><br></p>
</section><section id="law-of-large-numbers" class="level2" data-number="4.6"><h2 data-number="4.6" class="anchored" data-anchor-id="law-of-large-numbers">
<span class="header-section-number">4.6</span> Law of Large Numbers</h2>
<p>The <strong>Law of Large Numbers (LLN)</strong> is one of the fundamental results in probability theory that establishes the consistency of the sample mean. We can prove this important result by applying the sufficient condition for consistency.</p>
<p>Consider an i.i.d. sample <span class="math inline">\{Y_1, Y_2, \ldots, Y_n\}</span> with <span class="math inline">E[Y_i] = \mu</span> and <span class="math inline">Var(Y_i) = \sigma^2 &lt; \infty</span>. For the sample mean <span class="math inline">\overline{Y}_n = \frac{1}{n}\sum_{i=1}^n Y_i</span>, we can verify:</p>
<ol type="1">
<li><p><span class="math inline">E[\overline{Y}_n] = E\left[\frac{1}{n}\sum_{i=1}^n Y_i\right] = \frac{1}{n}\sum_{i=1}^n E[Y_i] = \frac{1}{n} \cdot n \cdot \mu = \mu</span>, so <span class="math inline">Bias[\overline{Y}_n] = 0</span> for all <span class="math inline">n</span></p></li>
<li><p><span class="math inline">Var[\overline{Y}_n] = Var\left[\frac{1}{n}\sum_{i=1}^n Y_i\right] = \frac{1}{n^2}\sum_{i=1}^n Var[Y_i] = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n} \to 0</span> as <span class="math inline">n \to \infty</span></p></li>
</ol>
<p>Therefore, <span class="math inline">MSE(\overline{Y}_n) = Var(\overline{Y}_n) + [Bias(\overline{Y}_n)]^2 = \frac{\sigma^2}{n} + 0 \to 0</span> as <span class="math inline">n \to \infty</span>.</p>
<p>Since the MSE approaches zero, by the sufficient condition for consistency, we conclude that <span class="math inline">\overline{Y}_n \overset{p}{\to} \mu</span>. This result is formalized as the Law of Large Numbers:</p>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>Law of Large Numbers (LLN)</strong></p>
<p>Let <span class="math inline">\{Y_1, Y_2, \ldots, Y_n\}</span> be a sequence of independent and identically distributed (i.i.d.) random variables with <span class="math inline">E[Y_i] = \mu</span> and <span class="math inline">Var(Y_i) = \sigma^2 &lt; \infty</span>. Then:</p>
<p><span class="math display">\overline{Y}_n = \frac{1}{n}\sum_{i=1}^n Y_i \overset{p}{\to} \mu \quad \text{as} \quad n \to \infty</span></p>
</div>
<p><br></p>
<p>The LLN essentially states that if we take a large enough sample from a population with finite mean, the sample mean will be close to the population mean with high probability.</p>
<p>Below is an interactive Shiny app to visualize the law of large numbers using simulated data for different sample sizes and different distributions.</p>
<div style="text-align: center; font-size: 150%;">
<p><a href="http://shiny2.svenotto.com:3838/sample-apps/LLN/">SHINY APP: LLN</a></p>
</div>
<p>The LLN extends to functions of sample means as well. If <span class="math inline">g(\cdot)</span> is a continuous function and <span class="math inline">\overline{Y}_n \overset{p}{\to} \mu</span>, then:</p>
<p><span class="math display">g(\overline{Y}_n) \overset{p}{\to} g(\mu)</span></p>
<p>This result, known as the <strong>continuous mapping theorem</strong>, allows us to establish consistency for a wide range of estimators that can be expressed as functions of sample means.</p>
</section><section id="rate-of-convergence" class="level2" data-number="4.7"><h2 data-number="4.7" class="anchored" data-anchor-id="rate-of-convergence">
<span class="header-section-number">4.7</span> Rate of Convergence</h2>
<p>While consistency tells us that an estimator eventually converges to the true parameter, it doesn’t indicate how quickly this convergence occurs. The <strong>rate of convergence</strong> provides this information.</p>
<p>We already learned that the MSE for the sample mean is <span class="math display">
  MSE(\overline{Y}) = \frac{\sigma^2}{n},
</span> where <span class="math inline">0 &lt; Var[Y] = \sigma^2 &lt; \infty</span>.</p>
<p>A quantity with better interpretability than the MSE is its square root, similar to the relationship between variance and standard deviation.</p>
<p>The <strong>root mean squared error (RMSE)</strong> of an estimator <span class="math inline">\widehat{\theta}</span> for <span class="math inline">\theta</span> is <span class="math display">
  RMSE(\widehat{\theta}) = \sqrt{MSE(\widehat{\theta})} = \sqrt{E[(\widehat{\theta} - \theta)^2]}.
</span></p>
<p>The RMSE measures how much an estimate differs on average from its true parameter value for a given sample size <span class="math inline">n</span>.</p>
<p>The RMSE of the sample mean is <span class="math display">
  RMSE(\overline{Y}) = \frac{\sigma}{\sqrt{n}}.
</span></p>
<p>Since the RMSE is proportional to <span class="math inline">1/\sqrt{n}</span>, we say that the sample mean has the <strong>rate of convergence</strong> <span class="math inline">\sqrt{n}</span>. We have <span class="math inline">\lim_{n \to \infty} \sqrt{n} \cdot RMSE(\overline{Y}) = \sigma</span>.</p>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>Rate of Convergence</strong></p>
<p>An estimator <span class="math inline">\widehat{\theta}</span> with <span class="math inline">\lim_{n \to \infty} MSE(\widehat{\theta}) = 0</span> has convergence rate <span class="math inline">\sqrt{n}</span> if <span class="math display">
  0 &lt; \lim_{n \to \infty} \Big( \sqrt{n} \cdot RMSE(\widehat{\theta}) \Big) &lt; \infty
</span></p>
<p>More generally, the rate of convergence is <span class="math inline">r_n</span> if <span class="math display">
  0 &lt; \lim_{n \to \infty} \Big( r_n \cdot RMSE(\widehat{\theta}) \Big) &lt; \infty.
</span></p>
</div>
<p><br></p>
<p>The rate <span class="math inline">\sqrt{n}</span> is the standard convergence rate for estimators and valid for most estimators we use in practice under mild conditions. If the rate of convergence is <span class="math inline">\sqrt{n}</span>, we say that the estimator has a <strong>parametric convergence rate</strong>. There are exceptions where estimators have slower or faster convergence rates (nonparametric estimators, bootstrap, cointegration, long-memory time series).</p>
<p>The rate of convergence gives a first indication of how fast the uncertainty decreases as we get more observations.</p>
<p>Consider the case of a <span class="math inline">\sqrt{n}</span> rate as in the sample mean case:</p>
<ul>
<li>To halve the average deviation of the estimate from the true parameter value, we need to increase the sample size by a factor of 4 since <span class="math inline">\sqrt{4}=2</span>.</li>
<li>To reduce the RMSE by a factor of 4, we already need to increase the sample size by a factor of 16.</li>
</ul>
<p>Thus, to achieve substantially smaller error margins, researchers must gather disproportionately larger samples.</p>
</section><section id="convergence-in-distribution" class="level2" data-number="4.8"><h2 data-number="4.8" class="anchored" data-anchor-id="convergence-in-distribution">
<span class="header-section-number">4.8</span> Convergence in Distribution</h2>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>Convergence in Distribution</strong></p>
<p>A sequence of random variables <span class="math inline">\{W_n\}_{n=1}^{\infty}</span> <strong>converges in distribution</strong> to a random variable <span class="math inline">W</span> if: <span class="math display">\lim_{n \to \infty} F_{n}(x) = F(x)</span> for all points <span class="math inline">x</span> where <span class="math inline">F(x)</span> is continuous. Here, <span class="math inline">F_{n}</span> and <span class="math inline">F</span> are the cumulative distribution functions of <span class="math inline">W_n</span> and <span class="math inline">W</span>, respectively.</p>
<p>This is denoted as <span class="math inline">W_n \overset{d}{\to} W</span>.</p>
</div>
<p><br></p>
<p>Unlike convergence in probability, which relates a sequence of random variables to a fixed constant, convergence in distribution relates the sequence to another random variable with a specific distribution.</p>
<p>If an estimator <span class="math inline">\widehat{\theta}_n</span> is consistent for <span class="math inline">\theta</span>, then: <span class="math display">\widehat{\theta}_n \overset{p}{\to} \theta</span></p>
<p>This implies that <span class="math inline">\widehat{\theta}_n</span> also converges in distribution to the constant <span class="math inline">\theta</span>: <span class="math display">\widehat{\theta}_n \overset{d}{\to} \theta</span></p>
<p>However, this limiting distribution is <strong>degenerate</strong>, meaning it places all probability mass at the single point <span class="math inline">\theta</span>. A degenerate distribution at a point <span class="math inline">c</span> satisfies <span class="math inline">P(W=c)=1</span>. Consequently, even though we write <span class="math inline">\widehat{\theta}_n \overset{d}{\to} \theta</span>, this tells us nothing about the shape of the sampling distribution for finite <span class="math inline">n</span>; it simply confirms that <span class="math inline">\widehat{\theta}_n</span> converges to <span class="math inline">\theta</span> in the limit.</p>
</section><section id="asymptotic-distribution-of-an-estimator" class="level2" data-number="4.9"><h2 data-number="4.9" class="anchored" data-anchor-id="asymptotic-distribution-of-an-estimator">
<span class="header-section-number">4.9</span> Asymptotic Distribution of an Estimator</h2>
<p>To obtain a non-degenerate limiting distribution that provides useful information about the sampling variability of a consistent estimator, we typically examine a standardized version of the estimator.</p>
<p>If <span class="math inline">\widehat{\theta}_n</span> converges to <span class="math inline">\theta</span> at rate <span class="math inline">r_n</span>, then we study: <span class="math display">r_n(\widehat{\theta}_n - \theta)</span></p>
<p>For many estimators with <span class="math inline">r_n = \sqrt{n}</span>, this standardized quantity converges in distribution to a normal random variable: <span class="math display">\sqrt{n}(\widehat{\theta}_n - \theta) \overset{d}{\to} \mathcal{N}(0, V)</span> where <span class="math inline">V</span> is the asymptotic variance.</p>
<p>The distribution of <span class="math inline">\sqrt{n}(\widehat{\theta}_n - \theta)</span> is called the <strong>asymptotic distribution</strong> of the estimator <span class="math inline">\widehat{\theta}_n</span>. For large <span class="math inline">n</span>, we can approximate: <span class="math display">\widehat{\theta}_n \approx \mathcal{N}\left(\theta, \frac{V}{n}\right)</span></p>
<p>This approximation is the basis for constructing confidence intervals and conducting hypothesis tests in large samples.</p>
</section><section id="central-limit-theorem" class="level2" data-number="4.10"><h2 data-number="4.10" class="anchored" data-anchor-id="central-limit-theorem">
<span class="header-section-number">4.10</span> Central Limit Theorem</h2>
<p>The <strong>Central Limit Theorem (CLT)</strong> is the key result that establishes the asymptotic normality of many estimators.</p>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>Central Limit Theorem (CLT)</strong></p>
<p>Let <span class="math inline">\{Y_1, Y_2, \ldots, Y_n\}</span> be a sequence of i.i.d. random variables with mean <span class="math inline">\mu</span> and variance <span class="math inline">\sigma^2 &lt; \infty</span>. Then: <span class="math display">\sqrt{n}(\overline{Y}_n - \mu) \overset{d}{\to} \mathcal{N}(0, \sigma^2)</span></p>
<p>Or equivalently: <span class="math display">\frac{\overline{Y}_n - \mu}{\sigma/\sqrt{n}} \overset{d}{\to} \mathcal{N}(0, 1)</span></p>
</div>
<p><br></p>
<p>The CLT tells us that the standardized sample mean follows a standard normal distribution in large samples, regardless of the underlying distribution of the individual observations (as long as the variance is finite).</p>
<p>This remarkable result means that we can construct approximate confidence intervals and conduct hypothesis tests for the mean using the normal distribution, even when the population distribution is non-normal, provided the sample size is sufficiently large.</p>
<p>The same result can be extended to <span class="math inline">k</span>-dimensional random vectors:</p>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>Multivariate Central Limit Theorem (MCLT)</strong></p>
<p>If <span class="math inline">\{\boldsymbol{W}_1, \ldots, \boldsymbol{W}_n\}</span> is an i.i.d. sample of <span class="math inline">k</span>-dimensional random vectors with <span class="math inline">E[\boldsymbol{W}_i] = \boldsymbol{\mu}</span> and <span class="math inline">Var(\boldsymbol{W}_i) = \boldsymbol{\Sigma} &lt; \infty</span>, then: <span class="math display">
\sqrt{n} \bigg( \frac{1}{n} \sum_{i=1}^n \boldsymbol{W}_i - \boldsymbol{\mu} \bigg) \overset{d}{\to} \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})
</span></p>
</div>
<p><br></p>
<p>Below, you will find an interactive shiny app for the central limit theorem:</p>
<div style="text-align: center; font-size: 150%;">
<p><a href="http://shiny2.svenotto.com:3838/sample-apps/CLT">SHINY APP: CLT</a></p>
</div>
<p>I’ll revise the section to follow more closely the approach in the paste.txt document while maintaining consistency with the previous parts.</p>
</section><section id="the-normal-distribution" class="level2" data-number="4.11"><h2 data-number="4.11" class="anchored" data-anchor-id="the-normal-distribution">
<span class="header-section-number">4.11</span> The Normal Distribution</h2>
<p>The <strong>Gaussian distribution</strong>, also known as the <strong>normal distribution</strong>, is a fundamental concept in statistics and plays a central role in asymptotic theory.</p>
<div style="border: 1px solid black; background-color:#f2f2f2; padding: 10px;">
<p><strong>Normal Distribution</strong></p>
<p>A random variable <span class="math inline">Z</span> follows a normal distribution with mean <span class="math inline">\mu</span> and variance <span class="math inline">\sigma^2</span>, denoted <span class="math inline">Z \sim \mathcal{N}(\mu, \sigma^2)</span>, if its probability density function (PDF) is: <span class="math display">f(u) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(u-\mu)^2}{2\sigma^2}\right)</span></p>
<p>The key moments of the normal distribution are:</p>
<ul>
<li>Mean: <span class="math inline">E[Z] = \mu</span>
</li>
<li>Variance: <span class="math inline">Var(Z) = \sigma^2</span>
</li>
<li>Skewness: <span class="math inline">ske(Z) = 0</span>
</li>
<li>Kurtosis: <span class="math inline">kur(Z) = 3</span>
</li>
</ul>
</div>
<p><br></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-Ndistribution" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-Ndistribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="part4_convergence_files/figure-html/fig-Ndistribution-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-Ndistribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: PDF and CDF of <span class="math inline">\mathcal N(2,2)</span>
</figcaption></figure>
</div>
</div>
</div>
<p>The Gaussian distribution with mean 0 and variance 1 is called the <strong>standard normal distribution</strong>. It has the PDF: <span class="math display">\phi(u) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{u^2}{2}\right)</span></p>
<p>and CDF: <span class="math display">\Phi(a) = \int_{-\infty}^a \phi(u) \ du</span></p>
<p>The standard normal distribution is symmetric around zero: <span class="math display">\phi(u) = \phi(-u), \quad \Phi(a) = 1 - \Phi(-a)</span></p>
<p><strong>Standardization</strong>: If <span class="math inline">Z \sim \mathcal{N}(\mu, \sigma^2)</span>, then: <span class="math display">\frac{Z-\mu}{\sigma} \sim \mathcal{N}(0,1)</span></p>
<p>and the CDF of <span class="math inline">Z</span> is <span class="math inline">\Phi((u-\mu)/\sigma)</span>.</p>
<p><strong>Linear combinations</strong>: If <span class="math inline">Y_1, \ldots, Y_n</span> are normally distributed and <span class="math inline">c_1, \ldots, c_n \in \mathbb{R}</span>, then <span class="math inline">\sum_{j=1}^n c_j Y_j</span> is normally distributed. In particular, if <span class="math inline">X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2)</span> and <span class="math inline">X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2)</span> are independent, then: <span class="math display">a X_1 + b X_2 \sim \mathcal{N}(a\mu_1 + b\mu_2, a^2\sigma_1^2 + b^2\sigma_2^2)</span></p>
<section id="multivariate-normal-distribution" class="level3" data-number="4.11.1"><h3 data-number="4.11.1" class="anchored" data-anchor-id="multivariate-normal-distribution">
<span class="header-section-number">4.11.1</span> Multivariate Normal Distribution</h3>
<p>Let <span class="math inline">Z_1, \ldots, Z_k</span> be independent <span class="math inline">\mathcal{N}(0,1)</span> random variables. Then, the <span class="math inline">k</span>-vector <span class="math inline">\boldsymbol{Z} = (Z_1, \ldots, Z_k)'</span> has the <strong>multivariate standard normal distribution</strong>, written <span class="math inline">\boldsymbol{Z} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I}_k)</span>. Its joint density is: <span class="math display">f(\boldsymbol{u}) = \frac{1}{(2\pi)^{k/2}} \exp\left(-\frac{\boldsymbol{u}'\boldsymbol{u}}{2}\right)</span></p>
<p>More generally, a random vector <span class="math inline">\boldsymbol{Z}^* = \boldsymbol{\mu} + \boldsymbol{B}\boldsymbol{Z}</span> (where <span class="math inline">\boldsymbol{\mu}</span> is a <span class="math inline">q \times 1</span> vector and <span class="math inline">\boldsymbol{B}</span> is a <span class="math inline">q \times k</span> matrix) has a <strong>multivariate normal distribution</strong> with mean vector <span class="math inline">\boldsymbol{\mu}</span> and covariance matrix <span class="math inline">\boldsymbol{\Sigma} = \boldsymbol{B}\boldsymbol{B}'</span>, written <span class="math inline">\boldsymbol{Z}^* \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})</span>. The <span class="math inline">k</span>-variate PDF is: <span class="math display">f(\boldsymbol{u}) = \frac{1}{(2\pi)^{k/2}(\det(\boldsymbol{\Sigma}))^{1/2}} \exp\left(-\frac{1}{2}(\boldsymbol{u}-\boldsymbol{\mu})'\boldsymbol{\Sigma}^{-1}(\boldsymbol{u}-\boldsymbol{\mu})\right)</span></p>
<p>The mean vector and covariance matrix are: <span class="math display">E[\boldsymbol{Z}^*] = \boldsymbol{\mu}, \quad Var(\boldsymbol{Z}^*) = \boldsymbol{\Sigma}</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-MVNdistribution" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-MVNdistribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="part4_convergence_files/figure-html/fig-MVNdistribution-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-MVNdistribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: PDF of a bivariate normal distribution
</figcaption></figure>
</div>
</div>
</div>
<p>The 3d plot shows the bivariate normal PDF with parameters <span class="math display">
  \boldsymbol \mu = \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \quad \boldsymbol \Sigma = \begin{pmatrix} 1 &amp; 0.8 \\ 0.8 &amp; 1 \end{pmatrix}.
</span></p>
</section><section id="chi-squared-distribution" class="level3" data-number="4.11.2"><h3 data-number="4.11.2" class="anchored" data-anchor-id="chi-squared-distribution">
<span class="header-section-number">4.11.2</span> Chi-squared Distribution</h3>
<p>Let <span class="math inline">Z_1, \ldots, Z_m</span> be independent <span class="math inline">\mathcal{N}(0,1)</span> random variables. Then, the random variable: <span class="math display">Y = \sum_{i=1}^m Z_i^2</span> is <strong>chi-squared distributed</strong> with parameter <span class="math inline">m</span> (degrees of freedom), written <span class="math inline">Y \sim \chi^2_m</span>.</p>
<p>The key moments are:</p>
<ul>
<li>Mean: <span class="math inline">E[Y] = m</span>
</li>
<li>Variance: <span class="math inline">Var(Y) = 2m</span>
</li>
<li>Skewness: <span class="math inline">ske(Y) = \sqrt{8/m}</span>
</li>
<li>Kurtosis: <span class="math inline">kur(Y) = 3+12/m</span>
</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-chisquare" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-chisquare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="part4_convergence_files/figure-html/fig-chisquare-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chisquare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: PDFs of the <span class="math inline">\chi^2</span>-distribution
</figcaption></figure>
</div>
</div>
</div>
</section><section id="students-t-distribution" class="level3" data-number="4.11.3"><h3 data-number="4.11.3" class="anchored" data-anchor-id="students-t-distribution">
<span class="header-section-number">4.11.3</span> Student’s t-Distribution</h3>
<p>If <span class="math inline">Z \sim \mathcal{N}(0,1)</span> and <span class="math inline">Q \sim \chi^2_m</span> are independent, then: <span class="math display">Y = \frac{Z}{\sqrt{Q/m}}</span> is <strong>t-distributed</strong> with <span class="math inline">m</span> degrees of freedom, written <span class="math inline">Y \sim t_m</span>.</p>
<p>The t-distribution with <span class="math inline">m=1</span> is also called the <strong>Cauchy distribution</strong>. The t-distributions with degrees of freedom 1-4 are heavy-tailed distributions. As <span class="math inline">m \to \infty</span>, the t-distribution approaches the standard normal distribution.</p>
<p>The key moments (when they exist) are: * Mean: <span class="math inline">E[Y] = 0</span> if <span class="math inline">m \geq 2</span> * Variance: <span class="math inline">Var(Y) = \frac{m}{m-2}</span> if <span class="math inline">m \geq 3</span> * Skewness: <span class="math inline">ske(Y) = 0</span> if <span class="math inline">m \geq 4</span> * Kurtosis: <span class="math inline">kur(Y) = 3+\frac{6}{m-4}</span> if <span class="math inline">m \geq 5</span></p>
<p>For smaller values of <span class="math inline">m</span>, certain moments do not exist: the kurtosis is infinite for <span class="math inline">m \leq 4</span>, the skewness is undefined for <span class="math inline">m \leq 3</span>, the variance is infinite for <span class="math inline">m \leq 2</span>, and the mean is undefined for <span class="math inline">m=1</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-studentt" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-studentt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="part4_convergence_files/figure-html/fig-studentt-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-studentt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.4: PDFs of the Student <span class="math inline">t</span>-distribution
</figcaption></figure>
</div>
</div>
</div>
</section><section id="f-distribution" class="level3" data-number="4.11.4"><h3 data-number="4.11.4" class="anchored" data-anchor-id="f-distribution">
<span class="header-section-number">4.11.4</span> F-Distribution</h3>
<p>If <span class="math inline">Q_1 \sim \chi^2_m</span> and <span class="math inline">Q_2 \sim \chi^2_r</span>, and if <span class="math inline">Q_1</span> and <span class="math inline">Q_2</span> are independent, then: <span class="math display">Y = \frac{Q_1/m}{Q_2/r}</span> is <strong>F-distributed</strong> with parameters <span class="math inline">m</span> and <span class="math inline">r</span>, written <span class="math inline">Y \sim F_{m,r}</span>.</p>
<p>The parameter <span class="math inline">m</span> is called the degrees of freedom in the numerator; <span class="math inline">r</span> is the degree of freedom in the denominator. If <span class="math inline">r \to \infty</span>, then the distribution of <span class="math inline">mY</span> approaches <span class="math inline">\chi^2_m</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-fdistribution" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-fdistribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="part4_convergence_files/figure-html/fig-fdistribution-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fdistribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.5: PDFs of the <span class="math inline">F</span>-distribution
</figcaption></figure>
</div>
</div>
</div>


</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./part3_covariance.html" class="pagination-link" aria-label="Multiple Random Variables">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple Random Variables</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>