[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability Theory for Econometricians",
    "section": "",
    "text": "Welcome\nThis tutorial gives a short introduction to the most important basic concepts of probability theory and statistics for econometricians.\nThis tutorial is still under construction. The two sections presented here are the first two sections of my course Statistics for Data Analytics from Winter Term 2023, which contains a review of probability theory.\nFor a quick review of the basics, I recommend sections 2 and 3 of Stock and Watson (2019): LINK",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "sec02_probability.html#random-experiments",
    "href": "sec02_probability.html#random-experiments",
    "title": "\n1  Probability\n",
    "section": "\n1.1 Random experiments",
    "text": "1.1 Random experiments\nA random experiment is a procedure or situation where the result is uncertain and determined by a probabilistic mechanism. An outcome is a specific result of a random experiment. The sample space S is the set/collection of all potential outcomes.\nLet’s consider some examples:\n\nCoin toss: The outcome of a coin toss can be ‘heads’ or ‘tails’. This random experiment has a two-element sample space: S = \\{heads, tails\\}.\nGender: If you conduct a survey and interview a random person to ask them about their gender, the answer may be ‘female’, ‘male’, or ‘diverse’. It is a random experiment since the person to be interviewed is selected randomly. The sample space has three elements: S = \\{female, male, diverse\\}.\nEducation level: If you ask a random person about their education level according to the ISCED-2011 framework, the outcome may be one of the eight ISCED-2011 levels. We have an eight-element sample space: S = \\{Level \\ 1, Level \\ 2, Level \\ 3, Level \\ 4, Level \\ 5, Level \\ 6, Level \\ 7, Level \\ 8\\}.\nWage: If you ask a random person about their income per working hour in EUR, there are infinitely many potential answers. Any (non-negative) real number may be an outcome. The sample space is a continuum of different wage levels.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "sec02_probability.html#random-variables",
    "href": "sec02_probability.html#random-variables",
    "title": "\n1  Probability\n",
    "section": "\n1.2 Random variables",
    "text": "1.2 Random variables\nA random variable is a numerical summary of a random experiment. In econometrics and applied statistics, we always express random experiments in terms of random variables. Let’s define some random variables based on the random experiments above:\n\n\nCoin: A two-element sample space random experiment can be transformed to a binary random variable, i.e., a random variable that takes either 0 or 1. We define the coin random variable as \nY = \\begin{cases}\n1   & \\text{if outcome is heads,} \\\\\n0   & \\text{if outcome is tails.}\n\\end{cases}\n A binary random variable is also called Bernoulli random variable.\n\n\n\n\n\n\nFigure 1.1: Bernoulli random variable\n\n\n\nFemale dummy: The three-element sample space of the gender random experiment does not provide any natural ordering. A useful way to transform it into random variables are dummy variables. The female dummy variable is a Bernoulli random variable with \nY = \\begin{cases}\n1   & \\text{if the person is female,} \\\\\n0   & \\text{if the person is not female.}\n\\end{cases}\n Similarly, dummy variables for male and diverse can be defined.\nEducation: The eight-element sample space of the education-level random experiment provides a natural ordering. We define the random variable education as the number of years of schooling of the interviewed person: \nY = \\text{number of years of schooling} \\in \\{4, 10, 12, 13, 14, 16, 18, 21\\}.\n\n\n\n\nTable 1.1: ISCED 2011 levels\n\n\n\n\nISCED level\nEducation level\nYears of schooling\n\n\n\n1\nPrimary\n4\n\n\n2\nLower Secondary\n10\n\n\n3\nUpper secondary\n12\n\n\n4\nPost-Secondary\n13\n\n\n5\nShort-Cycle Tertiary\n14\n\n\n6\nBachelor's\n16\n\n\n7\nMaster's\n18\n\n\n8\nDoctoral\n21\n\n\n\n\n\n\n\n\n\nWage: The wage level of the interviewed is already numerical. The random variable is \nY = \\text{income per working hour in EUR}.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "sec02_probability.html#probability-function",
    "href": "sec02_probability.html#probability-function",
    "title": "\n1  Probability\n",
    "section": "\n1.3 Probability function",
    "text": "1.3 Probability function\nIn the case of a fair coin, it is natural to assign the following probabilities to the coin variable: P(Y = 0) = 0.5 and P(Y = 1) = 0.5. By definition, the coin variable will never take the value 2.5, so the corresponding probability is P(Y=2.5) = 0. We may also consider intervals, e.g., P(Y \\geq 0) = 1 and P(-1 \\leq Y &lt; 1) = 0.5\nThe probability function P assigns values between 0 and 1 to events. Specific subsets of the real line define events. Any real number defines an event, and any open, half-open, or closed interval represents an event as well, e.g., \n  A_1 = \\{Y=0\\}, \\quad A_2 = \\{Y=1\\}, \\quad A_3 = \\{Y=2.5\\}\n and \n  A_4 = \\{Y \\geq 0\\}, \\quad A_5 = \\{ -1 \\leq Y &lt; 1 \\}.\n We may take complements \n  A_6 := A_4^c = \\{Y \\geq 0\\}^c = \\{ Y &lt; 0\\},\n as well as unions and intersections: \\begin{align*}\nA_7 &:= A_1 \\cup A_6 = \\{Y=0\\} \\cup \\{Y&lt; 0\\} = \\{Y \\leq 0\\}, \\\\\nA_8 &:= A_4 \\cap A_5 = \\{Y \\geq 0\\} \\cap \\{ -1 \\leq Y &lt; 1 \\} = \\{ 0 \\leq Y &lt; 1 \\}.\n\\end{align*} Unions and intersections can also applied iteratively, \n  A_9 := A_1 \\cup A_2 \\cup A_3 \\cup A_5 \\cup A_6 \\cup A_7 \\cup A_8 = \\{ Y \\in (-\\infty, 1] \\cup \\{2.5\\}\\},\n and by taking complements, we obtain the full real line and the empty set: \\begin{align*}\nA_{10} &:= A_9 \\cup A_9^c = \\{Y \\in \\mathbb R\\}, \\\\\nA_{11} &:= A_{10}^c = \\{\\}.\n\\end{align*} You may verify that P(A_1) = 0.5, P(A_2) = 0.5, P(A_3) = 0, P(A_4) = 1 P(A_5) = 0.5, P(A_6) = 0, P(A_7) = 0.5, P(A_8) = 0.5, P(A_9) = 1, P(A_{10}) = 1, P(A_{11}) = 0. If you take the variables education or wage, the probabilities of these events may be completely different.\nTo make probabilities a mathematically sound concept, we have to define to which events probabilities are assigned and how these probabilities are assigned. We consider the concept of a sigma algebra to collect all events.\n\nSigma algebra\nA collection \\mathcal B of sets is called sigma algebra if it satisfies the following three properties:\n\n\\{\\} \\in \\mathcal B  (empty set)\nIf A \\in \\mathcal B then A^c \\in \\mathcal B\nIf A_1, A_2, \\ldots \\in \\mathcal B, then A_1 \\cup A_2 \\cup \\ldots \\in \\mathcal B.\n\n\n\nIf you take all events of the form \\{ Y \\in (a,b) \\}, where a, b \\in \\mathbb R \\cup \\{-\\infty, \\infty\\}, and if you add all unions, intersections, and complements of these events, and again all unions, intersections, and complements of those events, and so on, you will obtain the so-called Borel sigma algebra. The Borel sigma algebra contains all events we assign probabilities to, the Borel sets.\nProbabilities must follow certain conditions. The following axioms ensure that these conditions are fulfilled:\n\nProbability function\nA probability function P is a function P: \\mathcal B \\to [0,1] that satisfies the Axioms of Probability:\n\nP(A) \\geq 0 for every A \\in \\mathcal B\nP(Y \\in \\mathbb R) = 1\nIf A_1, A_2, A_3 \\ldots are disjoint then A_1 \\cup A_2 \\cup A_3 \\cup \\ldots = P(A_1) + P(A_2) + P(A_3) + \\ldots\n\n\n\nRecall that two events A and B are disjoint if they have no outcomes in common, i.e., if A \\cap B = \\{\\}. For instance, A_1 and A_2 are A_1 = \\{Y=0\\} and A_2 = \\{Y=1\\} are disjoint, but A_1 and A_4 = \\{Y \\geq 0\\} are not disjoint, since A_1 \\cap A_4 = \\{Y=0\\} is nonempty.\nProbabilities are a well-defined concept if we use the Borel sigma algebra and the axioms of probability. The mathematical details are developed in the field of measure theory.\nThe axioms of probability imply the following rules of calculation:\n\nBasic rules of probability\n\n\n0 \\leq P(A) \\leq 1  for any event A\n\n\nP(A^c) = 1 - P(A)  for the complement event of A\n\n\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)  for any events A, B (inclusion-exclusion principle)\n\nP(A) \\leq P(B)  if A \\subset B\n\n\nP(A \\cup B) = P(A) + P(B)  if A and B are disjoint",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "sec02_probability.html#distribution",
    "href": "sec02_probability.html#distribution",
    "title": "\n1  Probability\n",
    "section": "\n1.4 Distribution",
    "text": "1.4 Distribution\nThe distribution of a random variable Y is characterized by the probabilities of all events of Y in the Borel sigma algebra. The distribution of the coin variable is fully characterized by the probabilities P(Y=1) = 0.5 and P(Y=0) = 0.5. We can compute the probabilities of all other events using the basic rules of probability. The probability mass function summarizes these probabilities:\n\nProbability mass function (PMF)\nThe probability mass function (PMF) of a random variable Y is \n  \\pi(a) := P(Y = a), \\quad a \\in \\mathbb R\n\n\n\nThe PMF of the coin variable is \n  \\pi(a) = P(Y=a) = \\begin{cases} 0.5 & \\text{if} \\ a \\in\\{0,1\\}, \\\\\n  0 & \\text{otherwise}. \\end{cases}\n The education variable may have the following PMF: \n        \\pi(a) = P(Y=a) = \\begin{cases}\n        0.008 & \\text{if} \\ a = 4 \\\\\n        0.048 & \\text{if} \\ a = 10 \\\\\n        0.392 & \\text{if} \\ a = 12 \\\\\n        0.072 & \\text{if} \\ a = 13 \\\\\n        0.155 & \\text{if} \\ a = 14 \\\\\n        0.071 & \\text{if} \\ a = 16 \\\\\n        0.225 & \\text{if} \\ a = 18 \\\\\n        0.029 & \\text{if} \\ a = 21 \\\\\n        0   & \\text{otherwise}\n        \\end{cases}\n  \nThe PMF is useful for distributions where the sum of the PMF values over a discrete (finite or countably infinite) number of domain points equals 1, as in the examples above. These distributions are called discrete distributions.\nAnother example of a discrete distribution is the Poisson distribution with parameter \\lambda &gt; 0, which has the PMF \n  \\pi(a) = \\begin{cases} \\frac{e^{-\\lambda} \\lambda^a}{a!} & \\text{if} \\ a = 0, 1,2,3, \\ldots \\\\\n  0 & \\text{otherwise.} \\end{cases}\n It has a countably infinite number of domain points with nonzero PMF values, and its probabilities sum to 1, i.e., \\sum_{a=0}^\\infty \\pi(a) = e^{-\\lambda} \\sum_{a=0}^\\infty \\frac{\\lambda^a}{a!} = 1 since the exponential function has the power series representation e^\\lambda = \\sum_{a=0}^\\infty \\frac{\\lambda^a}{a!}.\nNot all random variables are discrete, e.g., the wage variable takes values on a continuum. The cumulative distribution function is a unifying concept summarizing the distribution of any random variable.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "sec02_probability.html#cumulative-distribution-function",
    "href": "sec02_probability.html#cumulative-distribution-function",
    "title": "\n1  Probability\n",
    "section": "\n1.5 Cumulative distribution function",
    "text": "1.5 Cumulative distribution function\n\nCumulative distribution function (CDF)\nThe cumulative distribution function (CDF) of a random variable Y is \n  F(a) := P(Y \\leq a), \\quad a \\in \\mathbb R,\n\n\n\nThe CDF of the variable coin is \n        F(a) = \\begin{cases} 0 & a &lt; 0, \\\\\n        0.5 & 0 \\leq a &lt; 1, \\\\\n        1 & a \\geq 1, \\end{cases}\n     with the following CDF plot:\n\n\n\n\n\nFigure 1.2: CDF of coin\n\n\nThe CDF of the variables education is\n\n\n\n\n\nFigure 1.3: CDF of education\n\n\nand the CDF of the variable wage may have the following form:\n\n\n\n\n\nFigure 1.4: CDF of wage\n\n\nBy the basic rules of probability, we can compute the probability of any event if we know the probabilities of all events of the form \\{Y \\leq a\\}.\n\nSome basic rules for the CDF (for a &lt; b):\n\nP(Y \\leq a) = F(a)\nP(Y &gt; a) = 1 - F(a)\nP(Y &lt; a) = F(a) - \\pi(a)\nP(Y \\geq a) = 1 - P(Y &lt; a)\nP(a &lt; Y \\leq b) = F(b) - F(a)\nP(a &lt; Y &lt; b) = F(b) - F(a) - \\pi(b)\nP(a \\leq Y \\leq b) = F(b) - F(a) + \\pi(a)\nP(a \\leq Y &lt; b) = P(a \\leq Y \\leq b) - \\pi(b)\n\n\n\nSome CDFs have jumps/steps, and some CDFs are smooth/continuous. If F has a jump at domain point a, then the PMF at a is \n  \\pi(a) = P(Y=a) = F(a) - \\lim_{\\epsilon \\to 0} F(a-\\epsilon) = \\text{``jump height at} \\ a\\text{''.}\n\\tag{1.1} If F is continuous at domain point a, we have \\lim_{\\epsilon \\to 0} F(a-\\epsilon) = F(a), which implies that \\pi(a) = P(Y=a) = 0.\nWe call the random variable a discrete random variable if the CDF contains jumps and is flat between the jumps. A discrete random variable has only a finite (or countably infinite) number of potential outcomes. The values of the PMF correspond to the jump heights in the CDF as defined in Equation 1.1. The support \\mathcal Y of a discrete random variable Y is the set of all points a \\in \\mathbb R with nonzero probability mass, i.e. \\mathcal{Y} = \\{ a \\in \\mathbb{R} : \\pi(a) &gt; 0 \\}. The probabilities of a discrete random variable sum to 1, i.e., \\sum_{a \\in \\mathcal Y} \\pi(a)= 1.\nThe Bernoulli variables coin and female are discrete random variables with support \\mathcal Y = \\{0,1\\}. The variable eduaction has support \\mathcal Y = \\{4, 10, 12, 13, 14, 16, 18, 21\\}. A Poisson random variable has thr support \\mathcal Y = \\mathbb N \\cup \\{0\\}.\nWe call a random variable a continuous random variable if the CDF is continuous at every point a \\in \\mathbb R. A continuous random variable has \\pi(a) = P(Y=a) = 0 for all a \\in \\mathbb R. The basic rules for the CDF become simpler in the case of a continuous random variable:\n\nRules for the CDF of a continuous random variable (for a &lt; b):\n\nP(Y \\leq a) = P(Y &lt; a) = F(a)\nP(Y \\geq a) = P(Y &gt; a) = 1 - F(a)\nP(a &lt; Y \\leq b) = P(a \\leq Y &lt; b) = F(b) - F(a)\nP(a &lt; Y &lt; b) = P(a \\leq Y \\leq b) = F(b) - F(a)\n\n\n\nSingle-outcome events are null sets and occur with probability zero. Therefore, the PMF is not suitable to describe the distribution of a continuous random variable. We use the CDF to compute probabilities of interval events as well as their unions, intersections, and complements.\n\n\n\n\n\nFigure 1.5: CDF of wage evaluated at some points\n\n\nFor instance, P(Y \\leq 30) = 0.942, P(Y \\leq 20) = 0.779, P(Y \\leq 10) = 0.217, and P(10 \\leq Y \\leq 20) = 0.779 - 0.217 = 0.562.\n\nQuantiles\nFor a continuous random variable Y the \\alpha-quantile q(\\alpha) is defined as the solution to the equation \\alpha = F(q(\\alpha)), or, equivalently, as the inverse of the distribution function: \n        q(\\alpha) = F^{-1}(\\alpha)\n    \n\n\n\n\nq(\\cdot) is a function from (0,1) to \\mathbb R.\nSome quantiles have special names:\n\nThe median is the 0.5 quantile.\nThe quartiles are the 0.25, 0.5 and 0.75 quantiles.\nThe deciles are the 0.1, 0.2,… , 0.9 quantiles.\n\n\n\n\n\n\n\n\nFigure 1.6: Quantiles of variable wage\n\n\nFrom the quantile plot, we find that q(0.1) = 7.73, q(0.5) = 13.90, q(0.9) = 26.18. Under this wage distribution, the median wage is 13.90 EUR, the poorest 10% have a wage of less than 7.33 EUR, and the richest 10% have a wage of more than 26.18 EUR.\n\n\n\n\n\nFigure 1.7: Quantiles of variable education\n\n\nThe median of education is 13, the 0.1-quantile is 12, and the 0.9-quantile is 18.\nA CDF has the following properties:\n\nit is non-decreasing,\nit is right-continuous (jumps may occur only when the limit point is approached from the left)\nthe left limit is zero: \\lim_{a \\to -\\infty} F(a) = 0\n\nthe right limit is one: \\lim_{a \\to \\infty} F(a) = 1.\n\nAny function F that satisfies these four properties defines a probability distribution. Typically, distributions are divided into discrete and continuous distributions. Still, it may be the case that a distribution does not fall into either of these categories (for instance, if a CDF has jumps on some domain points and is continuously increasing on other domain intervals). In any case, the CDF characterizes the entire distribution of any random variable.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "sec02_probability.html#probability-density-function",
    "href": "sec02_probability.html#probability-density-function",
    "title": "\n1  Probability\n",
    "section": "\n1.6 Probability density function",
    "text": "1.6 Probability density function\nFor discrete random variables, both the PMF and the CDF characterize the distribution. In the case of a continuous random variable, the PMF does not yield any information about the distribution since it is zero. The continuous counterpart of the PMF is the density function:\n\nProbability density function\nThe probability density function (PDF) or simply density function of a continuous random variable Y is a function f(a) that satisfies \n    F(a) = \\int_{-\\infty}^a f(u) \\ \\text{d}u\n The density f(a) is the derivative of the CDF F(a) if it is differentiable: \n    f(a) = \\frac{d}{da} F(a).\n\n\n\nProperties of a PDF:\n\nf(a) \\geq 0 for all a \\in \\mathbb R\n\\int_{-\\infty}^\\infty f(u) \\ \\text{d}u = 1\n\n\n\n\n\n\nFigure 1.8: PDF of the variable wage\n\n\n\nProbability rule for the PDF: \n  P(a &lt; Y &lt; b) = \\int_a^b f(u) \\ \\text{d} u = F(b) - F(a)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "sec02_probability.html#expected-value",
    "href": "sec02_probability.html#expected-value",
    "title": "\n1  Probability\n",
    "section": "\n1.7 Expected value",
    "text": "1.7 Expected value\nThe expectation or expected value is the most important measure of the central tendency of a distribution. It gives you the average value you can expect to get if you repeat the random experiment multiple times. We define the expectation first for discrete random variables, then continuous random variables, and finally give a unified definition for all random variables.\n\n1.7.1 Expectation of a discrete random variable\n\nThe expectation or expected value of a discrete random variable Y with PMF \\pi(\\cdot) and support \\mathcal Y is defined as \n    E[Y] =\n    \\sum_{u \\in \\mathcal Y} u \\pi(u).\n\n\nFor the coin variable, we have \\mathcal Y = \\{0,1\\} and therefore \n  E[Y] = 0\\cdot\\pi(0) + 1\\cdot\\pi(1) = 0.5.\n For the variable education we get \\begin{align*}\n  E[Y] &= 4\\cdot\\pi(4) + 10\\cdot\\pi(10) + 12\\cdot \\pi(12) \\\\\n  &\\phantom{=} + 13\\cdot\\pi(13) + 14\\cdot\\pi(14) + 16\\cdot \\pi(16) \\\\\n  &\\phantom{=} + 18 \\cdot \\pi(18) + 21*\\pi(21) = 13.557\n\\end{align*}\nThe expectation of a Poisson distributed random variable Y with parameter \\lambda is \n  E[Y] = 0+ \\sum_{a=1}^\\infty a \\cdot e^{-\\lambda}\\frac{\\lambda^a}{a!} =\ne^{-\\lambda} \\sum_{a=1}^\\infty \\frac{\\lambda^a}{(a-1)!} = e^{-\\lambda} \\sum_{a=0}^\\infty \\frac{\\lambda^{a+1}}{a!} = \\lambda e^{-\\lambda} e^{\\lambda} = \\lambda.\n\n\n1.7.2 Expectation of a continuous random variable\n\nThe expectation or expected value of a of a continuous random variable Y with PDF f(\\cdot) is \n    E[Y] =\n    \\int_{-\\infty}^\\infty u f(u) \\ \\text{d}u.\n\n\nUsing numerical integration for the density of Figure 1.8 yields the expected value of 16.45 EUR for the wage variable, which is larger than the median value of 13.90 EUR. If the mean is larger than the median, we have a positively skewed distribution, meaning that a few people have high salaries, and many people have medium and low wages.\nThe uniform distribution on the unit interval [0,1] has the PDF \n  f(u) = \\begin{cases} 1 & \\text{if} \\ u \\in[0,1], \\\\ 0 & \\text{otherwise,} \\end{cases}\n and the expected value of a uniformly distributed random variable Y is \n  E[Y] = \\int_{-\\infty}^\\infty u f(u) \\ \\text{d} u = \\int_{0}^1 u \\ \\text{d} u = \\frac{1}{2}.\n\n\n1.7.3 Expectation for general random variables\nWe can also define the expected value in a unified way for any random variable so we do not have to distinguish between discrete and continuous random variables. Let F(\\cdot) be the CDF of the random variable of interest and consider the differential \\text{d} F(u), which corresponds to an infinitesimal change in F(\\cdot) at u. For a discrete random variable, F(u) changes only if there is a step/jump at u and zero otherwise because it is flat. Thus, for a discrete distribution, \n  \\text{d} F(u) = \\begin{cases} \\pi(u) & \\text{if} \\ u \\in \\mathcal Y \\\\\n  0 & \\text{if} \\ u \\notin \\mathcal Y. \\end{cases}\n In the case of a continuous random variable with differentiable CDF F(\\cdot), we have \n  \\text{d} F(u) = f(u) \\ \\text{d}u,\n where f(\\cdot) is the PDF of the random variable. This gives rise to the following unified definition of the expected value:\n\nThe expectation or expected value of any random variable with CDF F(\\cdot) is defined as \n  E[Y] =\n    \\int_{-\\infty}^\\infty u \\ \\text{d}F(u).\n\\tag{1.2}\n\n\nNote that Equation 1.2 is the Riemann-Stieltjes integral of a with respect to the function F(\\cdot). Recall that the Riemann integral of u with respect to u over the interval [-1,1] is \\int_{-1}^1 u \\ \\text{d} u := \\lim_{N\\to \\infty} \\sum_{j=1}^{2N} \\Big(\\frac{j}{N} - 1\\Big) \\Big(\\big(\\tfrac{j}{N} - 1\\big) - \\big(\\tfrac{j-1}{N} - 1 \\big)\\Big) = \\lim_{N\\to \\infty} \\sum_{j=1}^{2N} \\Big(\\frac{j}{N} - 1\\Big) \\frac{1}{N}, for the interval [-z,z] we have \\int_{-z}^z u \\ \\text{d} u := \\lim_{N\\to \\infty} \\sum_{j=1}^{2N} z\\Big(\\frac{j}{N} - 1\\Big) \\frac{z}{N}, and we obtain \\int_{-\\infty}^\\infty u \\ \\text{d} u := \\lim_{z \\to \\infty} \\int_{-z}^z u \\ \\text{d} u for the integral over the entire real line. Note that z/N = z(\\frac{j}{N}-1) - z(\\frac{j-1}{N}-1) corresponds to a change in u on [-z,z] so we approximate \n\\text{d} u \\approx z\\big(\\tfrac{j}{N}-1\\big) - z\\big(\\tfrac{j-1}{N}-1\\big) = \\tfrac{z}{N}\n and let N tend to infinity. In the case of the Riemann-Stieltjes integral, where we integrate with respect to changes in a function F(\\cdot), i.e., \\text{d} F(u). In an interval [-z,z], we have \\text{d} F(u) \\approx F\\Big(z\\big(\\tfrac{j}{N}-1\\big)\\Big) - F\\Big(z\\big(\\tfrac{j-1}{N}-1\\big)\\Big), and we define \\begin{align*}\n  \\int_{-z}^z u \\ \\text{d}F(u) &:= \\lim_{N \\to \\infty} \\sum_{j=1}^{2N} z\\Big(\\tfrac{j}{N} - 1\\Big) F\\Big(z\\big(\\tfrac{j}{N}-1\\big)\\Big) - F\\Big(z\\big(\\tfrac{j-1}{N}-1\\big)\\Big) \\\\\n  \\int_{-\\infty}^\\infty u \\ \\text{d}F(u) &:= \\lim_{z\\to \\infty} \\int_{-z}^z u \\ \\text{d}F(u)\n\\end{align*}\n\n1.7.4 Properties of the expected value\nThe expected value is a measure of central tendency. It is a linear function. For any two random variables Y and Z and any a,b \\in \\mathbb R, we have \n  E[aY + bZ] = a E[Y] + b E[Z].\n\nThe expected value has some optimality properties in terms of prediction. The best predictor of a random variable Y in the mean square error sense is the value g^* that minimizes E[(Y-g)^2] over g. We have \n  E[(Y-g)^2] = E[Y^2] -2gE[Y] + g^2,\n and minimizing over g yields \n  \\frac{\\text{d}E[(Y-g)^2]}{\\text{d}g} = -2E[Y] + 2g,\n which is zero if g=E[Y]. The second derivative is positive. Therefore, the expected value is the best predictor for a random variable if you do not have any further information available.\nWe often transform random variables by taking, for instance, squares Y^2 or logs \\log(Y). For any transformation function g(\\cdot), the expectation of the transformed random variable g(Y) is \n  E[g(Y)] = \\int_{-\\infty}^\\infty g(u) \\ \\text{d}F(u),\n where \\text{d}F(u) can be replaced by the PMF or the PDF as discussed in Section 1.7.3 for the different cases. For instance, if we take the coin variable Y and consider the transformed random variable \\log(Y+1), the expected value is \n  E[\\log(Y+1)] = \\log(1) \\cdot \\frac{1}{2} + \\log(2) \\cdot \\frac{1}{2} = \\frac{\\log(2)}{2}\n\n\nMoments\nThe r-th moment of a random variable Y is defined as \n    E[Y^r] = \\int_{-\\infty}^\\infty u^r \\ \\text{d}F(u) = \\begin{cases}\n      \\sum_{u \\in \\mathcal Y} u^r \\pi(u) & \\text{if} \\ Y \\ \\text{is discrete,} \\\\\n      \\int_{-\\infty}^\\infty u^r f(u)\\text{d}u & \\text{if} \\ Y \\ \\text{is continuous.} \\end{cases}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "sec02_probability.html#descriptive-features-of-a-distribution",
    "href": "sec02_probability.html#descriptive-features-of-a-distribution",
    "title": "\n1  Probability\n",
    "section": "\n1.8 Descriptive features of a distribution",
    "text": "1.8 Descriptive features of a distribution\n\nSome important features of the distribution of Y\n\n\n\n\n\n\n\n\nE[Y^r]\n\nr-th moment of Y\n\n\n\n\nE[(Y-E[Y])^r]\n\nr-th central moment of Y\n\n\n\n\nVar[Y] = E[(Y-E[Y])^2]\n\nvariance of Y\n\n\n\n\nsd(Y) = \\sqrt{Var[Y]}\n\nstandard deviation of Y\n\n\n\n\nE[((Y-E[Y])/sd(Y))^r]\n\nr-th standardized moment of Y\n\n\n\nskew = E[((Y-E[Y])/sd(Y))^3]\n\nskewness of Y\n\n\n\nkurt = E[((Y-E[Y])/sd(Y))^4]\n\nkurtosis of Y\n\n\n\n\nThe mean is a measure of central tendency and equals the expected value. The variance and standard deviation are measures of dispersion. We have \n  Var[Y] = E[(Y-E[Y])^2] = E[Y^2] - E[Y]^2\n and \n  Var[a+bY] = b^2 Var[Y]\n for any a,b \\in \\mathbb R. The skewness \n  skew = \\frac{E[(Y - E[Y])^3]}{sd(Y)^3} =\\frac{E[Y^3] - 3 E[Y^2] E[Y] + 2 E[Y]^3}{(E[Y^2] - E[Y]^2)^{3/2}}\n is a measure of asymmetry\n\n\n\n\n\nFigure 1.9: Positive and negative skewness\n\n\nA random variable Y has a symmetric distribution about 0 if F(u) = 1 - F(-u). If Y has a density, it is symmetric if f(x) = f(-x). If Y is symmetric about 0, then the skewness is 0. The skewness of the variable wage (see Figure 1.8) is positive, i.e., the distribution is positively skewed. The standard normal distribution \\mathcal N(0,1) , which has the density \n  f(u) = \\phi(u) = \\frac{1}{\\sqrt{2\\pi}} e^{-u^2/2}.\n\nBelow you find a plot of the PDFs of N(0,1) together with the t_5-distribution, which is the t-distribution with 5 degrees of freedom:\n\n\n\n\n\nFigure 1.10: PDFs of the standard normal distribution (solid) and the t_5-distribution (dashed)\n\n\nThe standard normal distribution and the t(5) distribution have skewness 0. The kurtosis \n  kurt = \\frac{E[(Y - E[Y])^4]}{sd(Y)^4} =\\frac{E[Y^4] - 4 E[Y^3] E[Y] + 6 E[Y^2]E[Y]^2 - 3 E[Y]^4 }{(E[Y^2] - E[Y]^2)^2}\n is a measure of how likely extreme outliers are. The standard normal distribution has kurtosis 3 and the t(5) distribution has kurtosis 9 so that outliers in t(5) are more likely than in \\mathcal N(0,1):\n\npar(mfrow=c(1,2), cex.main=1)\nplot(rnorm(1000), main = \"1000 simulated values of N(0,1)\", ylab = \"\")\nplot(rt(1000,5), main = \"1000 simulated values of t(5)\", ylab = \"\")\n\n\n\n\n\n\n\n\n\nThe kurtosis of the variable wage is also larger than 3, meaning outliers are much more likely than in the standard normal distribution. In this case, the positive skewness means that more people have a wage less than the average, and the large kurtosis means that there are very few people with exceptionally high salaries (outliers).\nAll features discussed above are functions of the first four moments E[Y], E[Y^2], E[Y^3] and E[Y^4].\n\n1.8.1 Heavy-tailed distributions\nExpectations might be infinity. For instance, the simple Pareto distribution has the PDF \n  f(a) = \\begin{cases} \\frac{1}{a^2} & \\text{if} \\ a &gt; 1, \\\\\n  0 & \\text{if} \\ a \\leq 1, \\end{cases}\n and the expected value is \n  E[X] = \\int_{-\\infty}^\\infty a f(a) \\ \\text{d}a\n  = \\int_{1}^\\infty  \\frac{1}{a} \\ \\text{d}a = \\log(a)|_1^\\infty = \\infty.\n The game of chance from the St. Petersburg paradox (see https://en.wikipedia.org/wiki/St._Petersburg_paradox) is an example of a discrete random variable with infinite expectation.\nThere are distributions with finite mean with some higher moments that are infinite. For instance, the first m-1 moments of the t_m distribution (Student’s-t distribution with m degrees of freedom) are finite, but the m-th moment and all higher order moments are infinite. Random variables with infinite first four moments have a so-called heavy-tailed distribution and may produce huge outliers. Many statistical procedures are only valid if the underlying distribution is not heavy-tailed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "sec02_probability.html#the-normal-distribution",
    "href": "sec02_probability.html#the-normal-distribution",
    "title": "\n1  Probability\n",
    "section": "\n1.9 The normal distribution",
    "text": "1.9 The normal distribution\nA random variable X is normally distributed with parameters (\\mu, \\sigma^2) if it has the density \n    f(a \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\Big( - \\frac{(a- \\mu)^2}{2 \\sigma^2} \\Big).\n   We write Y \\sim \\mathcal N(\\mu, \\sigma^2). Mean and variance are \n        E[Y] = \\mu, \\quad var[Y] = \\sigma^2.\n     Special case: standard normal distribution \\mathcal{N}(0,1) with density \n    \\phi(a) = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\Big( - \\frac{a^2}{2} \\Big)\n   and CDF \n    \\Phi(a) = \\int_{-\\infty}^a \\phi(u)\\text{d}u.\n   \\mathcal N(0,1) is symmetric around zero: \n    \\phi(a) = \\phi(-a), \\quad \\Phi(a) = 1 - \\Phi(-a)\n\n\npar(mfrow=c(1,2), bty=\"n\", lwd=1)\nx &lt;- seq(-5,9,by=0.01)\nplot(x,dnorm(x,2,2),ylab=\"\",xlab=\"\", type=\"l\", main= \"PDF of N(2,2)\")\nplot(x,pnorm(x,2,2),ylab=\"\",xlab=\"\", type=\"l\", main = \"CDF of N(2,2)\")\n\n\n\n\n\n\n\n\n\nIf Y_1, \\ldots, Y_n are normally distributed and c_1, \\ldots, c_n \\in \\mathbb R, then \\sum_{j=1}^n c_j Y_j is normally distributed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "sec02_probability.html#additional-reading",
    "href": "sec02_probability.html#additional-reading",
    "title": "\n1  Probability\n",
    "section": "\n1.10 Additional reading",
    "text": "1.10 Additional reading\n\nStock and Watson (2019), Section 2\nHansen (2022a), Section 1-2\nDavidson and MacKinnon (2004), Section 1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "sec02_probability.html#r-codes",
    "href": "sec02_probability.html#r-codes",
    "title": "\n1  Probability\n",
    "section": "\n1.11 R-codes",
    "text": "1.11 R-codes\nstatistics-sec2.R",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#multivariate-random-variables",
    "href": "sec03_dependence.html#multivariate-random-variables",
    "title": "\n2  Dependence\n",
    "section": "\n2.1 Multivariate random variables",
    "text": "2.1 Multivariate random variables\nIn statistics, we typically study multiple random variables simultaneously. We can collect k random variable X_1, \\ldots, X_k in a random vector \n      X = \\begin{pmatrix}\n      X_1 \\\\ \\vdots \\\\ X_k\n    \\end{pmatrix}=   (X_1, \\ldots, X_k)'.\n   We also call X a k-variate random variable.\nSince X is a random vector, its outcome is also vector-valued, e.g. X = x \\in \\mathbb R^k with x=(x_1, \\ldots, x_k)'. Events of the form \\{X \\leq x\\} mean that each component of the random vector X is smaller than the corresponding values of the vector x, i.e. \n\\{X \\leq x\\} = \\{X_1 \\leq x_1, \\ldots, X_k \\leq x_k\\}.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#bivariate-random-variables",
    "href": "sec03_dependence.html#bivariate-random-variables",
    "title": "\n2  Dependence\n",
    "section": "\n2.2 Bivariate random variables",
    "text": "2.2 Bivariate random variables\nIf k = 2, we call X a bivariate random variable. Consider, for instance, the coin toss Bernoulli variable Y with P(Y=1) = 0.5 and P(Y=0) = 0.5, and let Z be a second coin toss with the same probabilities. X = (Y,Z) is a bivariate random variable where both entries are discrete random variables. Since the two coin tosses are performed separately from each other, it is reasonable to assume that the probability that the first and second coin tosses show ‘heads’ is 0.25, i.e., P(\\{Y=1\\} \\cap \\{Z=1\\}) = 0.25. We would expect the following joint probabilities:\n\n\nTable 2.1: Joint probabilities of coin tosses\n\n\n\n\n\n\n\n\n\n\n\nZ=1\nZ=0\nany result\n\n\nY=1\n0.25\n0.25\n0.5\n\n\nY=0\n0.25\n0.25\n0.5\n\n\nany result\n0.5\n0.5\n1\n\n\n\n\n\n\nThe probabilities in the above table characterize the joint distribution of Y and Z. The table shows the values of the joint probability mass function: \n  \\pi_{YZ}(a,b) = \\begin{cases} 0.25 & \\text{if} \\ a \\in \\{0,1\\} \\ \\text{and} \\ b \\in \\{0,1\\} \\\\\n  0 & \\text{otherwise} \\end{cases}\n Another example are the random variables Y, a dummy variable for the event that the person has a high wage (more than 25 USD/hour), and Z, a dummy variable for the event that the same person has a university degree. Similarly, X = (Y,Z) is a bivariate random variable consisting of two univariate Bernoulli variables. The joint probabilities might be as follows:\n\n\nTable 2.2: Joint probabilities of wage and education dummies\n\n\n\n\n\n\n\n\n\n\n\nZ=1\nZ=0\nany education\n\n\nY=1\n0.19\n0.12\n0.31\n\n\nY=0\n0.17\n0.52\n0.69\n\n\nany wage\n0.36\n0.64\n1\n\n\n\n\n\n\nThe joint probability mass function is \n  \\pi_{YZ}(a,b) = \\begin{cases}\n      0.19 & \\text{if} \\ a=1, b=1, \\\\\n      0.12 & \\text{if} \\ a=1, b=0, \\\\\n      0.17 & \\text{if} \\ a=0, b=1, \\\\\n      0.52 & \\text{if} \\ a=0, b=0, \\\\\n      0   & \\text{otherwise.}\n  \\end{cases}\n The marginal probability mass function of Y is \\begin{align*}\n  \\pi_Y(a) &= P(Y=a) = \\pi_{YZ}(a,0) + \\pi_{YZ}(a,1) \\\\\n  &= \\begin{cases}\n  0.19 + 0.12 = 0.31 & \\text{if} \\ a = 1, \\\\\n  0.17 + 0.52 = 0.69 & \\text{if} \\ a = 0, \\\\\n  0 & \\text{otherwise.}\n  \\end{cases}\n\\end{align*} and the marginal probability mass function of Z is \\begin{align*}\n  \\pi_Z(b) &= P(Z=b) = \\pi_{YZ}(0,b) + \\pi_{YZ}(1,b) \\\\\n  &= \\begin{cases}\n  0.19 + 0.17 = 0.36 & \\text{if} \\ b = 1, \\\\\n  0.12 + 0.52 = 0.64 & \\text{if} \\ b = 0, \\\\\n  0 & \\text{otherwise.}\n  \\end{cases}\n\\end{align*}\nAn example of a continuous bivariate random variable is X = (Y,Z), where Y is the wage level in EUR/hour and Z is the labor market experience of the same person measured in years.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#bivariate-distributions",
    "href": "sec03_dependence.html#bivariate-distributions",
    "title": "\n2  Dependence\n",
    "section": "\n2.3 Bivariate distributions",
    "text": "2.3 Bivariate distributions\n\nBivariate distribution\nThe joint distribution function of a bivariate random variable (Y,Z) is\nF_{YZ}(a, b) = P(Y \\leq a, Z \\leq b) = P(\\{Y \\leq a\\} \\cap \\{ Z \\leq b\\}).\n\n\n\n\n\n\nFigure 2.1: Joint CDF of wage and experience\n\n\nCalculation of probabilities using a bivariate distribution function: \\begin{align*}\n        P(Y \\leq a, Z \\leq b) &= F_{YZ}(a,b) \\\\\n        P(a &lt; Y \\leq b, c &lt; Z \\leq d) &= F_{YZ}(b,d) - F_{YZ}(b,c) - F_{YZ}(a,d) + F_{YZ}(a,c)\n\\end{align*}\n\n\n\n\n\nFigure 2.2: Calculate probabilities using the joint CDF\n\n\n\n\n\n\n\nFigure 2.3: Calculate probabilities using the joint CDF\n\n\n\nMarginal distributions\nThe marginal distributions of Y and Z are \\begin{align*}\n        F_Y(a) = P(Y \\leq a) &= P(Y \\leq a, Z &lt; \\infty) &= \\lim_{b \\to \\infty} F_{YZ}(a,b),\\\\\n        F_Z(b) = P(Z \\leq b) &= P(Y &lt; \\infty, Z \\leq b) &= \\lim_{a \\to \\infty} F_{YZ}(a,b)\n\\end{align*}\n\n\n\n\n\n\n\nFigure 2.4: Marginal CDF of experience\n\n\n\n\n\n\n\nFigure 2.5: Marginal CDF of wage\n\n\n\nBivariate density function\nThe joint density function of a bivariate continuous random variable (Y,Z) with differentiable joint CDF F_{YZ}(a,b) equals \n    f_{YZ}(a,b) = \\frac{\\partial^2}{\\partial a \\partial b} F_{YZ}(a,b).\n\n\n\nThe marginal densities of Y and Z are \\begin{align*}\nf_Y(a) &= \\frac{d}{d a} F_Y(a) = \\int_{-\\infty}^\\infty f_{YZ}(a,b)\\text{d}b, \\\\\nf_Z(b) &= \\frac{d}{d b} F_Z(b) =        \\int_{-\\infty}^\\infty f_{YZ}(a,b)\\text{d}a.\n\\end{align*}\n\n\n\n\n\nFigure 2.6: Joint CDF of wage and experience\n\n\n\n\n\n\n\nFigure 2.7: Joint PDF of wage and experience",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#correlation",
    "href": "sec03_dependence.html#correlation",
    "title": "\n2  Dependence\n",
    "section": "\n2.4 Correlation",
    "text": "2.4 Correlation\nConsider the bivariate continuous random variable (Y,Z) with joint density f_{YZ}(a,b). The expected value of g(Y,Z), where g(\\cdot, \\cdot) is any real-valued function, is given by \n  E[g(X,Y)] = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty g(a,b) f_{YZ}(a,b) \\ \\text{d}a \\ \\text{d}b.\n\nThe first cross moment of Y and Z is E[YZ]. We have E[YZ] = E[g(Y,Z)] for the function g(Y,Z) = Y\\cdot Z. Therefore, \n      E[YZ] = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty ab f_{YZ}(a,b) \\ \\text{d}a \\ \\text{d}b.\n   The covariance of Y and Z is defined as\n   Cov(Y,Z) = E[(Y- E[Y])(Z-E[Z])] = E[YZ] - E[Y]E[Z].\n The covariance of Y and Y is the variance: \n  Cov(Y,Y) = Var[Y].\n The variance of the sum of two random variables depends on the covariance: \n  Var[Y+Z] = Var[Y] + 2 Cov(Y,Z) + Var[Z]\n The correlation of Y and Z is \n  Corr(Y,Z) = \\frac{Cov(Y,Z)}{sd(Y) sd(Z)}\n\n\nUncorrelated\nY and Z are uncorrelated if Corr(Y,Z) = 0, or, equivalently, if Cov(Y,Z) = 0.\n\n\nIf Y and Z are uncorrelated, we have \\begin{align*}\n        E[YZ] &= E[Y] E[Z] \\\\\n        var[Y+Z] &= var[Y] + var[Z]\n\\end{align*}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#independence",
    "href": "sec03_dependence.html#independence",
    "title": "\n2  Dependence\n",
    "section": "\n2.5 Independence",
    "text": "2.5 Independence\nTwo events A and B are independent if \n        P[A \\cap B] = P[A] P[B].\n For instance, in the bivariate random variable of Table 2.1 (two coin tosses), we have P(Y=1, Z=1) = 0.25 = 0.5 \\cdot 0.5 = P(Y=1)P(Z=1). Hence, \\{Y=1\\} and \\{Z=1\\} are independent events. In the bivariate random variable of Table 2.2 (wage/education), we find P(Y=1, Z=1) = 0.19 \\neq P(Y=1)P(Z=1) = 0.31 \\cdot 0.36 = 0.1116. Therefore, the two events are not independent. In this case, the two random variables are dependent.\n\nIndependence\nY and Z are independent random variables if, for all a and b, the bivariate distribution function is the product of the marginal distribution functions: \n        F_{YZ}(a,b) = F_Y(a) F_Z(b).\n     If this property is not satisfied, we say that X and Y are dependent.\n\n\nThe random variables Y and Z of Table 2.1 are independent, and those of Table 2.2 are dependent.\nIf Y and Z are independent and have finite second moments, then Y and Z are uncorrelated. The reverse is not true!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#random-vectors",
    "href": "sec03_dependence.html#random-vectors",
    "title": "\n2  Dependence\n",
    "section": "\n2.6 Random vectors",
    "text": "2.6 Random vectors\nThe above concepts can be generalized to any k-variate random vector X = (X_1, \\ldots, X_k). The joint CDF of X is \n    F_X(x) = P(X_1 \\leq x_1, \\ldots, X_k \\leq x_k).\n X has independent entries if \n        F_X(x) = \\prod_{i=1}^k P(X_i \\leq x_i) = \\prod_{i=1}^k F_{X_i}(x_i)\n     If F_X(x) is a continuous CDF, the joint k-dimensional density is \n    f_X(x) = f_X(x_1, \\ldots, x_k) = \\frac{\\partial^k}{\\partial x_1 \\cdots \\partial x_k} F_X(x_1, \\ldots ,x_k).\n  \nThe expectation vector of X is \n        E[X] = \\begin{pmatrix} E[X_1] \\\\  \\vdots \\\\ E[X_k] \\end{pmatrix},\n     and the covariance matrix of X is \\begin{align*}\nVar[X] &= E[(X-E[X])(X-E[X])'] \\\\\n      &= \\begin{pmatrix}\n            Var[X_1] & Cov(X_1, X_2) & \\ldots & Cov(X_1, X_k) \\\\\n            Cov(X_2, X_1) & Var[X_2] & \\ldots & Cov(X_2, X_k) \\\\\n            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            Cov(X_k, X_1) & Cov(X_k, X_2) & \\ldots & Var[X_k]\n        \\end{pmatrix}\n\\end{align*}\nFor any random vector X, the covariance matrix Var[X] is symmetric and positive semi-definite.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#conditional-distributions",
    "href": "sec03_dependence.html#conditional-distributions",
    "title": "\n2  Dependence\n",
    "section": "\n2.7 Conditional distributions",
    "text": "2.7 Conditional distributions\n\nConditional probability\nThe conditional probability of an event A given an event B with P(B) &gt; 0 is \n  P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\n\n\nLet’s revisit the wage and schooling example from Table 2.2: \n     P(Y=1 \\mid Z=1) = \\frac{P(\\{Y=1\\}\\cap \\{Z=1\\})}{P(Z=1)} = \\frac{0.19}{0.36} = 0.53\n     \n     P(Y=1 \\mid Z=0) = \\frac{P(\\{Y=1\\}\\cap \\{Z=0\\})}{P(Z=0)} = \\frac{0.12}{0.64} = 0.19\n    \nNote that \nP(Y=1 \\mid Z=1) = 0.53 &gt; 0.31 = P(Y=1)\n implies \nP(\\{Y=1\\}\\cap \\{Z=1\\}) &gt; P(Y=1) \\cdot P(Z=1).\n If P(A \\mid B) = P(A), then the events A and B are independent. If P(A \\mid B) \\neq P(A), they are dependent.\n\nConditional distribution of continuous variables\nConsider the density f_{YZ}(a,b) of two continuous random variables Y and Z. The conditional density of Y given Z=b is \n        f_{Y|Z}(a \\mid b) = \\frac{f_{YZ}(a,b)}{f_Z(b)}.\n The conditional distribution of Y given Z=b is \n    F_{Y|Z}(a \\mid b)  = \\int_0^a f_{Y|Z}(u \\mid b) \\ \\text{d}u.\n\n\n\n\n\n\n\n\nFigure 2.8: Joint PDF of wage and experience\n\n\n\n\n\n\n\nFigure 2.9: Conditional PDFs of wage given experience\n\n\n\n\n\n\n\nFigure 2.10: PDF of variable experience\n\n\nIf Y is continuous and Z is discrete, the conditional distribution function of Y given \\{Z=b\\} with P(Z=b) &gt; 0 is \n      F_{Y|Z}(a \\mid b) = P(Y \\leq a \\mid Z=b) = \\frac{P(Y \\leq a, Z=b)}{P(Z=b)}.\n  \nIf F_{Y|Z}(a \\mid b) is differentiable with respect to b, the conditional density of Y given Z=b is \n        f_{Y|Z}(a \\mid b) = \\frac{\\partial}{\\partial a} F_{Y|Z}(a \\mid b).\n    \n\n\n\n\n\nFigure 2.11: Conditional CDFs of wage given education\n\n\n\n\n\n\n\nFigure 2.12: Conditional PDFs of wage given education\n\n\nWe often are interested in conditioning on multiple variables, such as the wage given a particular education and experience level. Let f(y,x) = f(y,x_1, \\ldots, x_k) be the joint density of the composite random vector (Y, X_1, \\ldots, X_k) with X = (X_1, \\ldots, X_k). The conditional density of a random variable Y given X = x = (x_1, \\ldots, x_k)' is \n        f_{Y|X}(y \\mid x) = f(y \\mid x_1, \\ldots, x_k) = \\frac{f(y, x_1, \\ldots, x_k)}{f_X(x_1, \\ldots, x_k)} = \\frac{f(y,x)}{f_X(x)}\n     The conditional distribution of Y given X=x is \n        F_{Y|X}(y\\mid x) = \\int_0^y f(u \\mid x)\\ \\text{d}u.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#conditional-expectation",
    "href": "sec03_dependence.html#conditional-expectation",
    "title": "\n2  Dependence\n",
    "section": "\n2.8 Conditional expectation",
    "text": "2.8 Conditional expectation\n\nConditional expectation function\nThe conditional expectation of Y given X=x is the expected value of the distribution F_{Y|X}(y \\mid x). For continuous Y with conditional density f_{Y|X}(y \\mid x), the conditional expectation is\n\n    E[Y \\mid X=x] = \\int_{-\\infty}^\\infty y f_{Y|X}(y \\mid x)\\ \\text{d}y.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CEF wage given experience\n\n\n\n\n\n\n\n\n\n(b) CEF wage given education\n\n\n\n\n\n\nFigure 2.13: Conditional expectation functions\n\n\nConsider again the wage and experience example. Suppose that the conditional expectation has the functional form \n    E[wage \\mid experience = x ] = m(x) = 14.5 + 0.9 x - 0.017 x^2.\n E.g., for x=10 we have E[wage \\mid experience = 10] = m(10) = 21.8.\nNote that m(x) = E[wage \\mid experience = x] is not random. It is a feature of the joint distribution.\nSometimes, it is useful not to fix the experience level to a certain value but to treat it as random:\n\\begin{align*}\n      E[wage \\mid experience] &= m(experience) \\\\\n      &= 14.5 + 0.9 experience - 0.017 experience^2\n\\end{align*}\nm(experience) = E[wage \\mid experience] is a function of the random variable experience and, therefore, itself a random variable.\nThe conditional expectation function (CEF) of Y given the specific event \\{X=x\\} is \n      m(x) = E[Y \\mid X=x].\n   m(x) is deterministic (non-random) and a feature of the joint distribution.\nThe conditional expectation function (CEF) of Y given the random vector X is \n      m(X) = E[Y \\mid X].\n   m(X) is a function of the random vector X and therefore itself a random variable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#law-of-iterated-expectations",
    "href": "sec03_dependence.html#law-of-iterated-expectations",
    "title": "\n2  Dependence\n",
    "section": "\n2.9 Law of iterated expectations",
    "text": "2.9 Law of iterated expectations\n\nRules of calculation for the conditional expectation \nLet Y be a random variable and X a random vector.\n\nLaw of the iterated expectations (LIE): \nE[E[Y \\mid X]] = E[Y].\n   A more general LIE: For any two random vectors X and \\widetilde X, \n  E[E[Y \\mid X, \\widetilde X] \\mid X] = E[Y \\mid X].\n  \nConditioning theorem (CT): For any function g(\\cdot), \nE[g(X) Y \\mid X] = g(X) E[Y \\mid X].\n  \nIf Y and X are independent then E[Y \\mid X] = E[Y].",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#conditional-variance",
    "href": "sec03_dependence.html#conditional-variance",
    "title": "\n2  Dependence\n",
    "section": "\n2.10 Conditional variance",
    "text": "2.10 Conditional variance\n\nConditional variance\nIf E[Y^2] &lt; \\infty, the conditional variance of Y given the event \\{X=x\\} is \n            Var[Y \\mid X=x] = E[(Y-E[Y \\mid X=x])^2 \\mid X=x].\n        \nThe conditional variance of Y given the random vector X is \n    Var[Y \\mid X] = E[(Y-E[Y \\mid X])^2 \\mid X].",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#best-predictor",
    "href": "sec03_dependence.html#best-predictor",
    "title": "\n2  Dependence\n",
    "section": "\n2.11 Best predictor",
    "text": "2.11 Best predictor\nA typical application is to find a good prediction for the outcome of a random variable Y. Recall that the expected value E[Y] is the best predictor for Y in the sense that g^* = E[Y] minimizes E[(Y-g)^2].\nWith the knowledge of an additional random vector X, we can use the joint distribution of Y and X to improve the prediction of Y.\nIt turns out that the CEF m(X) = E[Y \\mid X] is the best predictor for Y given the information contained in the random vector X:\n\nBest predictor\nIf E[Y^2] &lt; \\infty, then the CEF m(X) = E[Y \\mid X] minimizes the expected squared error E[(Y-g(X))^2] among all predictor functions g(X).\n\n\nLet us find the function g(\\cdot) that minimizes E[(Y-g(X))^2]:\n\\begin{align*}\n        &E[(Y-g(X))^2] = E[(Y-m(X) + m(X) - g(X))^2] \\\\\n        &= \\underbrace{E[(Y-m(X))^2]}_{=(i)} + 2\\underbrace{E[(Y-m(X))(m(X) - g(X))]}_{=(ii)} + \\underbrace{E[(m(X) - g(X))^2]}_{(iii)}\n\\end{align*}\nThe first term (i) does not depend on g(\\cdot) and is finite if E[Y^2] &lt; \\infty.\nFor the second term (ii), we use the LIE and CT: \\begin{align*}\n        &E[(Y-m(X))(m(X) - g(X))] \\\\\n        &= E[E[(Y-m(X))(m(X) - g(X))\\mid X]] \\\\\n        &= E[E[Y-m(X)\\mid X](m(X) - g(X))] \\\\\n        &= E[(\\underbrace{E[Y\\mid X]}_{=m(X)} - m(X))(m(X) - g(X))] = 0\n\\end{align*}\nThe third term (iii) E[(m(X) - g(X))^2] is minimal if m(\\cdot) = g(\\cdot)\nTherefore, m(X) = E[Y\\mid X] minimizes E[(Y-g(X))^2].\nThe best predictor for Y given X is m(X)= E[Y \\mid X], but Y can typically only partially be predicted. We have a prediction error (CEF error) \n        e = Y - E[Y \\mid X].\n     The conditional expectation of the CEF error does not depend on X and is zero: \\begin{align*}\n        E[e \\mid X] &= E[(Y - m(X)) \\mid X] \\\\\n        &= E[Y \\mid X] - E[m(X) \\mid X] \\\\\n        &= m(X) - m(X) = 0\n\\end{align*}\nWe say that Y is conditional mean independent of Z if E[Y \\mid Z] does not depend on Z.\nIf Y and Z are independent, they are also conditional mean independent, but not necessarily vice versa. If Y and Z are conditional mean independent, they are also uncorrelated, but not necessarily vice versa.\nSince the CEF is the best predictor of Y, it is of great interest to study the CEF in practice. Much of the statistical and econometric research deals with methods to approximate and estimate the CEF. This field of statistics is called regression analysis.\nConsider the following model for Y and X: \n    Y = m(X) + e, \\quad E[e \\mid X] = 0.\n   \\tag{2.1} We call m(\\cdot) regression function and e error term.\nFrom equation Equation 2.1 it follows that \n    E[Y \\mid X] = E[m(X) + e \\mid X] = E[m(X) \\mid X] + E[e \\mid X] = m(X).\n     I.e., the nonparametric regression model is a model for the CEF.\nIf m(\\cdot) is a linear function, then Equation 2.1 is a linear regression model. We will study this model in detail in the next sections.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#combining-normal-variables",
    "href": "sec03_dependence.html#combining-normal-variables",
    "title": "\n2  Dependence\n",
    "section": "\n2.12 Combining normal variables",
    "text": "2.12 Combining normal variables\nSome of the distributions commonly encountered in econometrics are combinations of univariate normal distributions, such as the multivariate normal, chi-squared, Student t, and F distributions.\n\n2.12.1 \\chi^2-distribution\nLet Z_1, \\ldots, Z_m be independent \\mathcal N(0,1) random variables. Then, the random variable \n        Y = \\sum_{i=1}^m Z_i^2\n   is chi-square distributed with parameter m, written Y \\sim \\chi^2_m.\nThe parameter m is called the degrees of freedom.\nExpectation and variance: \n        E[Y] = m, \\quad var[Y] = 2m\n    \n\n\n\n\n\n\n\nFigure 2.14: \\chi^2 -distribution\n\n\n\n\n\n2.12.2 F-distribution\nIf Q_1 \\sim \\chi^2_m and Q_2 \\sim \\chi^2_r, and if Q_1 and Q_2 are independent, then \n        Y = \\frac{Q_1/m}{Q_2/r}\n     is F-distributed with parameters m and r, written Y \\sim F_{m,r}.\nThe parameter m is called the degrees of freedom in the numerator; r is the degree of freedom in the denominator.\nIf r \\to \\infty then the distribution of mY approaches \\chi^2_m\n\n\n\n\n\n\n\nFigure 2.15: F-distribution\n\n\n\n\n\n2.12.3 Student t-distribution\nIf Z \\sim \\mathcal N(0,1) and Q \\sim \\chi^2_m, and Z and Q are independent, then \n        Y = \\frac{Z}{\\sqrt{Q/m}}\n     is t-distributed with parameter m degrees of freedom, written Y \\sim t_m.\nExpectation, variance, and moments: \n        E[Y] = 0 \\quad  (\\text{if} \\ m \\geq 2),\n     \n        var[Y] = \\frac{m}{m-2} \\quad (\\text{if} \\ m \\geq 3)\n     The first m-1 moments are finite: E[|Y|^r] &lt; \\infty for r \\leq m-1 and E[|Y|^r] = \\infty for r \\geq m.\nThe t-distribution with m=1 is also called Cauchy distribution. The t-distributions with 1, 2, 3, and 4 degrees of freedom are heavy-tailed distributions. If m \\to \\infty then t_m \\to \\mathcal N(0,1)\n\n\n\n\n\n\n\nFigure 2.16: Student t-distribution\n\n\n\n\n\n2.12.4 Multivariate normal distribution\nLet X_1, \\ldots, X_k be independent \\mathcal N(0,1) random variables. Then, the k-vector X = (X_1, \\ldots, X_k)' has the multivariate standard normal distribution, written X \\sim \\mathcal N(\\boldsymbol 0, \\boldsymbol I_k). Its joint density is \n        f(x) = \\frac{1}{(2 \\pi)^{k/2}} \\exp\\left( - \\frac{x'x}{2} \\right).\n    \nIf X \\sim \\mathcal N(\\boldsymbol 0, \\boldsymbol I_k) and \\widetilde X = \\mu + \\boldsymbol B X for a q \\times 1 vector \\mu and a q \\times k matrix \\boldsymbol B, then \\widetilde X has a multivariate normal distribution with parameters \\mu and \\Sigma = \\boldsymbol B \\boldsymbol B', written \\widetilde X \\sim \\mathcal N(\\mu, \\Sigma). Its joint density is \n        f(x) = \\frac{1}{(2 \\pi)^{k/2} (\\det(\\Sigma))^{1/2} } \\exp\\Big(- \\frac{1}{2}(x-\\mu)'\\Sigma^{-1} (x-\\mu) \\Big).\n\nThe expectation vector and covariance matrix are \n        E[\\widetilde X] = \\mu, \\quad var[\\widetilde X] = \\Sigma.\n    \n\n2.12.5 R-commands for parametric distributions\n\n\n\n\n\n\n\n\n\nget CDFF(a)\n\nquantile functionq(p)\n\ngenerate n independent\nrandom numbers\n\n\n\n\\mathcal N(0,1)\npnorm(a)\nqnorm(p)\nrnorm(n)\n\n\n\\chi^2_r\npchisq(a,r)\nqchisq(p,r)\nrchisq(n,r)\n\n\nt_r\npt(a,r)\nqt(p,r)\nrt(n,r)\n\n\nF_{r,k}\npf(a,r,k)\nqf(p,r,k)\nrf(n,r,k)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  },
  {
    "objectID": "sec03_dependence.html#additional-reading",
    "href": "sec03_dependence.html#additional-reading",
    "title": "\n2  Dependence\n",
    "section": "\n2.13 Additional reading",
    "text": "2.13 Additional reading\n\nStock and Watson (2019), Section 2\nHansen (2022a), Section 4\nHansen (2022b), Section 2\nDavidson and MacKinnon (2004), Section 1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dependence</span>"
    ]
  }
]