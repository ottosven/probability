[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability Theory for Econometricians",
    "section": "",
    "text": "Welcome\nThis tutorial provides a concise introduction to the fundamental concepts of probability theory for econometricians and data scientists.\nCurrent Status: This tutorial is still under development.\nThe sections presented here originate from the Statistics for Data Analytics course taught in Winter Term 2024.\nFor a quick foundational review, I recommend sections 2 and 3 of Stock and Watson (2019): Textbook Link",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "part1_distribution.html",
    "href": "part1_distribution.html",
    "title": "\n1  Probability Distribution\n",
    "section": "",
    "text": "1.1 Random Experiment\nFrom an empirical perspective, a dataset is just a fixed array of numbers. Any summary statistic we compute – like a sample mean, sample correlation, or OLS coefficient – is simply a function of these numbers.\nThese statistics provide a snapshot of the data at hand but do not automatically reveal broader insights about the world. To add deeper meaning to these numbers, identify dependencies, and understand causalities, we must consider how the data were obtained.\nA random experiment is an experiment whose outcome cannot be predicted with certainty. In statistical theory, any dataset is viewed as the result of such a random experiment. While individual outcomes are unpredictable, patterns emerge when experiments are repeated.\nThe gender of the next person you meet, daily fluctuations in stock prices, monthly music streams of your favorite artist, or the annual number of pizzas consumed – all involve a certain amount of randomness and emerge from random experiments. Probability theory gives us the tools to analyze this randomness systematically.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "part1_distribution.html#random-variables",
    "href": "part1_distribution.html#random-variables",
    "title": "\n1  Probability Distribution\n",
    "section": "\n1.2 Random Variables",
    "text": "1.2 Random Variables\nA random variable is a numerical summary of a random experiment. An outcome is a specific result of a random experiment. The sample space S is the set/collection of all potential outcomes.\nLet’s consider some examples:\n\nCoin toss: The outcome of a coin toss can be “heads” or “tails”. This random experiment has a two-element sample space: S = \\{heads, tails\\}. We can express the experiment as a binary random variable: \nY = \\begin{cases}\n1   & \\text{if outcome is heads,} \\\\\n0   & \\text{if outcome is tails.}\n\\end{cases}\n\nGender: If you conduct a survey and interview a random person to ask them about their gender, the answer may be “female”, “male”, or “diverse”. It is a random experiment since the person to be interviewed is selected randomly. The sample space has three elements: S = \\{female, male, diverse\\}. To focus on female vs. non-female, we can define the female dummy variable: \nY = \\begin{cases}\n1   & \\text{if the person is female,} \\\\\n0   & \\text{if the person is not female.}\n\\end{cases}\n Similarly, dummy variables for male and diverse can be defined.\nEducation level: If you ask a random person about their education level according to the ISCED-2011 framework, the outcome may be one of the eight ISCED-2011 levels. We have an eight-element sample space: S = \\{Level \\ 1, Level \\ 2, Level \\ 3, Level \\ 4, Level \\ 5, Level \\ 6, Level \\ 7, Level \\ 8\\}.\n\nThe eight-element sample space of the education-level random experiment provides a natural ordering. We define the random variable education as the number of years of schooling of the interviewed person, with values corresponding to typical completion times in the German education system: \n  Y = \\text{years of schooling} \\in \\{4, 10, 12, 13, 14, 16, 18, 21\\}.\n\n\n\n\nTable 1.1: ISCED 2011 levels\n\n\n\n\nISCED level\nEducation level\nYears of schooling\n\n\n\n1\nPrimary\n4\n\n\n2\nLower Secondary\n10\n\n\n3\nUpper secondary\n12\n\n\n4\nPost-Secondary\n13\n\n\n5\nShort-Cycle Tertiary\n14\n\n\n6\nBachelor’s\n16\n\n\n7\nMaster’s\n18\n\n\n8\nDoctoral\n21\n\n\n\n\n\n\n\n\n\n\nWage: If you ask a random person about their income per working hour in EUR, there are infinitely many potential answers. Any (non-negative) real number may be an outcome. The sample space is a continuum of different wage levels. The wage level of the interviewed person is already numerical. The random variable is \nY = \\text{income per working hour in EUR}.\n\n\n\nRandom variables share the characteristic that their value is uncertain before conducting a random experiment (e.g., flipping a coin or selecting a random person for an interview). Their value is always a real number and is determined only once the experiment’s outcome is known.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "part1_distribution.html#events-and-probabilities",
    "href": "part1_distribution.html#events-and-probabilities",
    "title": "\n1  Probability Distribution\n",
    "section": "\n1.3 Events and Probabilities",
    "text": "1.3 Events and Probabilities\nTo quantify the uncertainty in random variables, we need to assign probabilities to different possible outcomes or sets of outcomes. This is where events and probability functions come into play.\nAn event of a random variable Y is a specific subset of the real line. Any real number defines an event (elementary event), and any open, half-open, or closed interval represents an event as well.\nLet’s define some specific events, using our coin toss example where Y=1 represents heads and Y=0 represents tails:\n\nElementary events: \\begin{align*}\nA_1 &= \\{Y=0\\} \\text{ (coin shows tails)} \\\\\nA_2 &= \\{Y=1\\} \\text{ (coin shows heads)} \\\\\nA_3 &= \\{Y=2.5\\} \\text{ (impossible outcome)}\n\\end{align*}\nHalf-open events: \\begin{align*}\nA_4 &= \\{Y \\geq 0\\} = \\{ Y \\in [0,\\infty) \\} \\\\\nA_5 &= \\{ -1 \\leq Y &lt; 1 \\} = \\{ Y \\in [-1,1) \\}\n\\end{align*}\n\nThe probability function P assigns values between 0 and 1 to events. For a fair coin toss (where Y=1 represents heads and Y=0 represents tails), it is natural to assign the following probabilities: \n  P(A_1) = P(Y=0) = 0.5, \\quad P(A_2) = P(Y=1) = 0.5\n\nBy definition, the coin variable will never take the value 2.5, so we assign \n  P(A_3) = P(Y=2.5) = 0\n\nTo assign probabilities to interval events, we check whether the elementary events \\{Y=0\\} and/or \\{Y=1\\} are subsets of the event of interest:\n\nIf both \\{Y=0\\} and \\{Y=1\\} are contained in the event of interest, the probability is 1\nIf only one of them is contained, the probability is 0.5\nIf neither is contained, the probability is 0\n\nFor our examples: \n  P(A_4) = P(Y \\geq 0) = 1, \\quad P(A_5) = P(-1 \\leq Y &lt; 1) = 0.5\n\nEvery event has a complementary event (denoted with superscript c), which consists of all outcomes not in the original event. For any pair of events, we can also take the union (denoted by \\cup) and intersection (denoted by \\cap). Let’s define further events:\n\nComplement (all outcomes not in the original event): \nA_6 = A_4^c = \\{Y \\geq 0\\}^c = \\{Y &lt; 0\\} = \\{Y \\in (-\\infty, 0)\\}\n\nUnion (outcomes in either event): \nA_7 = A_1 \\cup A_6 = \\{Y=0\\} \\cup \\{Y&lt; 0\\} = \\{Y \\leq 0\\}\n\nIntersection (outcomes in both events): \nA_8 = A_4 \\cap A_5 = \\{Y \\geq 0\\} \\cap \\{-1 \\leq Y &lt; 1\\} = \\{0 \\leq Y &lt; 1\\}\n\nCombinations of multiple events: \\begin{align*}\nA_9 &= A_1 \\cup A_2 \\cup A_3 \\cup A_5 \\cup A_6 \\cup A_7 \\cup A_8 \\\\\n&= \\{Y \\in (-\\infty, 1] \\cup \\{2.5\\}\\}\n\\end{align*}\nCertain event (contains all possible outcomes): \nA_{10} = A_9 \\cup A_9^c = \\{Y \\in (-\\infty, \\infty)\\} = \\{Y \\in \\mathbb{R}\\}\n\nEmpty event (contains no outcomes): \nA_{11} = A_{10}^c = \\{Y \\notin \\mathbb{R}\\} = \\{\\}\n\n\nFor the coin toss experiment, we can verify the probabilities of all these events:\n\n\nP(A_1) = 0.5 (probability of tails)\n\nP(A_2) = 0.5 (probability of heads)\n\nP(A_3) = 0 (coin never shows 2.5)\n\nP(A_4) = 1 (coin always shows a non-negative value)\n\nP(A_5) = 0.5 (only tails falls in this interval)\n\nP(A_6) = 0 (coin never shows a negative value)\n\nP(A_7) = 0.5 (same as probability of tails)\n\nP(A_8) = 0.5 (contains only tails)\n\nP(A_9) = 1 (contains all possible coin outcomes)\n\nP(A_{10}) = 1 (the certain event always occurs)\n\nP(A_{11}) = 0 (the empty event never occurs)\n\nTo illustrate how events and probabilities apply in other contexts, consider our education level example. If Y represents years of schooling with possible values \\{4, 10, 12, 13, 14, 16, 18, 21\\}, we might define the event B = \\{Y \\geq 16\\} representing “has at least a Bachelor’s degree.” The probability P(B) would then represent the proportion of the population with at least a Bachelor’s degree.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "part1_distribution.html#probability-function",
    "href": "part1_distribution.html#probability-function",
    "title": "\n1  Probability Distribution\n",
    "section": "\n1.4 Probability Function",
    "text": "1.4 Probability Function\nNow that we have defined events, we need a formal way to assign probabilities to them consistently. The probability function P assigns probabilities to events within the Borel sigma-algebra (denoted as \\mathcal B), which contains all events we would ever need to compute probabilities for in practice. This includes our previously mentioned events A_1, \\ldots, A_{11}, any interval of the form \\{ Y \\in (a,b) \\} with a, b \\in \\mathbb{R}, and all possible unions, intersections, and complements of these events.\nTwo events A and B are disjoint if A \\cap B = \\{\\}, meaning they have no common outcomes. For example, A_1 = \\{Y=0\\} and A_2 = \\{Y=1\\} are disjoint (a coin cannot show both heads and tails simultaneously), while A_1 and A_4 = \\{Y \\geq 0\\} are not disjoint since A_1 \\cap A_4 = \\{Y=0\\}.\nA probability function P must satisfy certain fundamental rules (axioms) to ensure a well-defined probability framework:\n\nBasic Rules of Probability\nFundamental Axioms:\n\n\nP(A) \\geq 0 for any event A (non-negativity)\n\nP(Y \\in \\mathbb R) = 1 for the certain event (normalization)\n\nP(A \\cup B) = P(A) + P(B) if A and B are disjoint (additivity)\n\nImplied Properties:\n\n\nP(Y \\notin \\mathbb R) = P(\\{\\}) = 0 for the empty event\n\n0 \\leq P(A) \\leq 1 for any event A\n\n\nP(A) \\leq P(B) if A is a subset of B (monotonicity)\n\nP(A^c) = 1 - P(A) for the complement event of A\n\n\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B) for any events A, B\n\n\n\n\nThe first three properties listed above are known as the axioms of probability, first formalized by Andrey Kolmogorov in 1933. The remaining properties follow as logical consequences of these axioms.\nLet’s consider a practical example: In our education survey, suppose we know the following probabilities:\n\nP(\\text{Primary education}) = 0.1\nP(\\text{Secondary education}) = 0.6\nP(\\text{Tertiary education}) = 0.3\n\nThese events are disjoint (a person cannot simultaneously have exactly primary and exactly secondary education as their highest level), and they cover all possibilities (everyone has some highest level of education). Using the axioms:\n\nEach probability is non-negative (satisfying axiom 1)\nThe sum 0.1 + 0.6 + 0.3 = 1 (satisfying axiom 2)\nThe probability of having either primary or secondary education is P(\\text{Primary or Secondary}) = P(\\text{Primary}) + P(\\text{Secondary}) = 0.1 + 0.6 = 0.7 (using axiom 3 for disjoint events)\n\nFrom the implied properties, we can also calculate that the probability of not having tertiary education is P(\\text{No tertiary}) = 1 - P(\\text{Tertiary}) = 1 - 0.3 = 0.7.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "part1_distribution.html#distribution-function",
    "href": "part1_distribution.html#distribution-function",
    "title": "\n1  Probability Distribution\n",
    "section": "\n1.5 Distribution Function",
    "text": "1.5 Distribution Function\nAssigning probabilities to events is straightforward for binary variables, like coin tosses. For instance, knowing that P(Y = 1) = 0.5 allows us to derive the probabilities for all events in \\mathcal B.\nHowever, for more complex variables, such as education or wage, defining probabilities for all possible events becomes more challenging due to the vast number of potential set operations involved.\nFortunately, it turns out that knowing the probabilities of events of the form \\{Y \\leq a\\} is enough to determine the probabilities of all other events. These probabilities are summarized in the cumulative distribution function.\n\nCumulative Distribution Function (CDF)\nThe cumulative distribution function (CDF) of a random variable Y is \n  F(a) := P(Y \\leq a), \\quad a \\in \\mathbb R.\n\n\n\nThe CDF is sometimes simply referred to as the distribution function, or the distribution.\nThe CDF of the variable coin is \n  F(a) = \\begin{cases}\n    0 & a &lt; 0, \\\\\n    0.5 & 0 \\leq a &lt; 1, \\\\\n    1 & a \\geq 1,\n  \\end{cases}\n\\tag{1.1}\nwith the following CDF plot:\n\n\n\n\n\nFigure 1.1: CDF of coin (discrete random variable)\n\n\nThe CDF of the variable education could be:\n\n\n\n\n\nFigure 1.2: CDF of education (discrete random variable)\n\n\nand the CDF of the variable wage may have the following form:\n\n\n\n\n\nFigure 1.3: CDF of wage (continuous random variable)\n\n\nNotice the key difference: the CDF of a continuous random variable (like wage) is smooth, while the CDF of a discrete random variable (like coin and education) contains jumps and is flat between these jumps. The height of each jump corresponds to the probability of that specific value occurring.\nAny function F(a) with the following properties defines a valid probability distribution:\n\nNon-decreasing: F(a) \\leq F(b) for a \\leq b.\nReflects the monotonicity of probability when the event \\{Y \\leq a\\} is contained in \\{Y \\leq b\\} for a &lt; b.\nLimits at 0 and 1: \\displaystyle \\lim_{a \\to -\\infty} F(a) = 0 and \\displaystyle \\lim_{a \\to \\infty} F(a) = 1.\nEnsures the total probability equals 1 and impossible events have zero probability.\nRight-continuity: \\displaystyle \\lim_{\\varepsilon \\to 0, \\varepsilon &gt; 0} F(a + \\varepsilon) = F(a).\nEnsures P(Y \\leq a) includes P(Y = a), which matters especially for discrete variables with jumps in the CDF. This property means that at any point a, the CDF value includes the probability mass exactly at that point, making F(a) = P(Y \\leq a) rather than P(Y &lt; a).\n\nBy the basic rules of probability, we can compute the probability of any event of interest if we know the CDF F(a). Here are the most common calculations:\n\nProbability Calculations Using the CDF (for a &lt; b):\n\nP(Y \\leq a) = F(a)\nP(Y &gt; a) = 1 - F(a)\nP(Y &lt; a) = F(a) - P(Y=a)\nP(Y \\geq a) = 1 - P(Y &lt; a)\nP(a &lt; Y \\leq b) = F(b) - F(a)\nP(a &lt; Y &lt; b) = F(b) - F(a) - P(Y=b)\nP(a \\leq Y \\leq b) = F(b) - F(a) + P(Y=a)\nP(a \\leq Y &lt; b) = F(b) - F(a)\n\n\n\nThe point probability P(Y = a) represents the size of the jump at a in the CDF F(a): \nP(Y=a) = F(a) - \\lim_{\\varepsilon \\to 0, \\varepsilon &gt; 0} F(a-\\varepsilon),\n which is the jump height at a. For continuous random variables, point probabilities are always zero, while for discrete random variables, they can be positive.\nHere, \\lim_{\\varepsilon \\to 0, \\varepsilon &gt; 0} F(a - \\varepsilon) denotes the left limit at a while \\lim_{\\varepsilon \\to 0, \\varepsilon &gt; 0} F(a+\\varepsilon) denotes the right limit at a. When approaching any point from the left, the CDF can have a jump at that point, while when approaching from the right, the CDF cannot jump (due to right-continuity).\nLet’s use our coin toss example to illustrate how to calculate different probabilities using the CDF in Equation 1.1:\n\nP(Y \\leq 0.5) = F(0.5) = 0.5\nP(Y &gt; 0.5) = 1 - F(0.5) = 1 - 0.5 = 0.5\nP(Y = 0) = F(0) - \\lim_{\\varepsilon \\to 0, \\varepsilon &gt; 0} F(0-\\varepsilon) = 0.5 - 0 = 0.5\nP(-1 &lt; Y \\leq 2) = F(2) - F(-1) = 1 - 0 = 1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "part1_distribution.html#probability-mass-function",
    "href": "part1_distribution.html#probability-mass-function",
    "title": "\n1  Probability Distribution\n",
    "section": "\n1.6 Probability Mass Function",
    "text": "1.6 Probability Mass Function\nIn the previous section, we defined the point probability P(Y = a) as the height of the jump in the CDF at point a. These point probabilities are systematically organized in the probability mass function:\n\nProbability Mass Function (PMF)\nThe probability mass function (PMF) of a random variable Y is \n  \\pi(a) := P(Y = a), \\quad a \\in \\mathbb R\n\n\n\nThe PMF of the coin variable is \n  \\pi(a) = P(Y=a) = \\begin{cases}\n    0.5 & \\text{if} \\ a \\in\\{0,1\\}, \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\n\n\n\n\n\n\n\n\n\n(a) CDF of coin\n\n\n\n\n\n\n\n\n\n(b) PMF of coin\n\n\n\n\n\n\nFigure 1.4: Coin variable: CDF (left) and PMF (right)\n\n\nThe education variable has the following PMF: \n        \\pi(a) = P(Y=a) = \\begin{cases}\n        0.008 & \\text{if} \\ a = 4 \\\\\n        0.055 & \\text{if} \\ a = 10 \\\\\n        0.393 & \\text{if} \\ a = 12 \\\\\n        0.079 & \\text{if} \\ a = 13 \\\\\n        0.145 & \\text{if} \\ a = 14 \\\\\n        0.078 & \\text{if} \\ a = 16 \\\\\n        0.218 & \\text{if} \\ a = 18 \\\\\n        0.024 & \\text{if} \\ a = 21 \\\\\n        0   & \\text{otherwise}\n        \\end{cases}\n  \nNote that these probability values sum to 1.\n\n\n\n\n\n\n\n\n\n(a) CDF of education\n\n\n\n\n\n\n\n\n\n(b) PMF of education\n\n\n\n\n\n\nFigure 1.5: Education variable: CDF (left) and PMF (right)\n\n\nThe support \\mathcal Y of Y is the set of all values that Y can take with non-zero probability: \\mathcal{Y} = \\{ a \\in \\mathbb{R} : \\pi(a) &gt; 0 \\}.\nFor the coin variable, the support is \\mathcal{Y} = \\{0, 1\\}, while for the education variable, the support is \\mathcal{Y} = \\{4, 10, 12, 13, 14, 16, 18, 21\\}.\nAny valid PMF must satisfy the following properties:\n\n\nNon-negativity: \\pi(a) \\geq 0 for all a \\in \\mathbb{R}\n\n\nSum to one: \\sum_{a \\in \\mathcal{Y}} \\pi(a) = 1\n\n\nRelationship to CDF: F(b) = \\sum_{a \\in \\mathcal{Y}, a \\leq b} \\pi(a)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "part1_distribution.html#probability-density-function",
    "href": "part1_distribution.html#probability-density-function",
    "title": "\n1  Probability Distribution\n",
    "section": "\n1.7 Probability Density Function",
    "text": "1.7 Probability Density Function\nFor continuous random variables, the CDF has no jumps, meaning the probability of any specific value is zero, and probability is distributed continuously over intervals. Unlike discrete random variables, which are characterized by both the PMF and the CDF, continuous variables do not have a positive PMF. Instead, they are described by the probability density function (PDF), which serves as the continuous analogue. If the CDF is differentiable, the PDF is given by its derivative:\n\nProbability Density Function (PDF)\nThe probability density function (PDF) or simply density function of a continuous random variable Y is the derivative of its CDF: \n    f(a) = \\frac{d}{da} F(a).\n\nConversely, the CDF can be obtained from the PDF by integration: \n    F(a) = \\int_{-\\infty}^a f(u) \\ \\text{d}u\n\n\n\nAny function f(a) with the following properties defines a valid probability density function:\n\n\nNon-negativity: f(a) \\geq 0 for all a \\in \\mathbb R\n\n\nNormalization: \\int_{-\\infty}^\\infty f(u) \\ \\text{d}u = 1\n\n\nThe support of a continuous random variable Y with PDF f is the set \\mathcal{Y} = \\{a \\in \\mathbb{R} : f(a) &gt; 0\\}, which contains all values where the density is positive. For instance, the support of the wage variable is \\mathcal{Y} = \\{a \\in \\mathbb{R} : a \\geq 0\\}, reflecting that wages cannot be negative.\n\n\n\n\n\n\n\n\n\n(a) CDF of wage\n\n\n\n\n\n\n\n\n\n(b) PDF of wage\n\n\n\n\n\n\nFigure 1.6: Wage variable: CDF (left) and PDF (right)\n\n\n\nBasic Rules for Continuous Random Variables (with a \\leq b):\n\n\\displaystyle P(Y = a) = \\int_a^a f(u) \\ \\text{d}u = 0\n\\displaystyle P(Y \\leq a) = P(Y &lt; a) = F(a) = \\int_{-\\infty}^a f(u) \\ \\text{d}u\n\\displaystyle P(Y &gt; a) = P(Y \\geq a) = 1 - F(a) = \\int_a^\\infty f(u) \\ \\text{d}u\n\\displaystyle P(a &lt; Y &lt; b) = F(b) - F(a) = \\int_a^b f(u) \\ \\text{d}u\n\\displaystyle P(a &lt; Y &lt; b) = P(a &lt; Y \\leq b) = P(a \\leq Y \\leq b) = P(a \\leq Y &lt; b)\n\n\n\nUnlike the PMF, which directly gives probabilities, the PDF does not represent probability directly. Instead, the probability is given by the area under the PDF curve over an interval. The PDF value f(a) itself can be greater than 1, as long as the total area under the curve equals 1.\nIt is important to note that for continuous random variables, the probability of any single point is zero. This is why, as shown in the last rule above, the inequalities (strict or non-strict) don’t affect the probability calculations for intervals. This stands in contrast to discrete random variables, where the inclusion of endpoints can change the probability value.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "part1_distribution.html#conditional-distribution",
    "href": "part1_distribution.html#conditional-distribution",
    "title": "\n1  Probability Distribution\n",
    "section": "\n1.8 Conditional Distribution",
    "text": "1.8 Conditional Distribution\nThe distribution of wage may differ between men and women. Similarly, the distribution of education may vary between married and unmarried individuals. In contrast, the distribution of a coin flip should remain the same regardless of whether the person tossing the coin earns 15 or 20 EUR per hour.\nThe conditional cumulative distribution function (CCDF), F_{Y|Z=b}(a) = F_{Y|Z}(a|b) = P(Y\\leq a|Z=b), represents the distribution of a random variable Y given that another random variable Z takes a specific value b. It answers the question: “If we know that Z=b, what is the distribution of Y?”\nFor example, suppose that Y represents wage and Z represents education:\n\n\nF_{Y|Z=12}(a) is the CDF of wages among individuals with 12 years of education.\n\nF_{Y|Z=14}(a) is the CDF of wages among individuals with 14 years of education.\n\nF_{Y|Z=18}(a) is the CDF of wages among individuals with 18 years of education.\n\nSince wage is a continuous variable, its conditional distribution given any specific value of another variable is also continuous. The conditional density of Y given Z=b is defined as the derivative of the conditional CDF: \n        f_{Y|Z=b}(a) = f_{Y|Z}(a|b) = \\frac{d}{d a} F_{Y|Z=b}(a).\n\n\n\n\n\n\n\n\n\n\n(a) Conditional CDFs of wage given education\n\n\n\n\n\n\n\n\n\n(b) Conditional PDFs of wage given education\n\n\n\n\n\n\nFigure 1.7: Wage distributions conditional on education level\n\n\nWe observe that the distribution of wage varies across different levels of education. For example, individuals with fewer years of education are more likely to earn less than 20 EUR per hour: \nP(Y\\leq 20 | Z=12) = F_{Y|Z=12}(20) &gt; F_{Y|Z=18}(20) = P(Y\\leq 20|Z = 18).\n Because the conditional distribution of Y given Z=b depends on the value of Z=b, we say that the random variables Y and Z are dependent random variables.\nNote that the conditional CDF F_{Y|Z=b}(a) can only be defined for values of b in the support of Z.\nWe can also condition on more than one variable. Let Z_1 represent the labor market experience in years and Z_2 be the female dummy variable. The conditional CDF of Y given Z_1 = b and Z_2 = c is: \nF_{Y|Z_1=b,Z_2=c}(a) = F_{Y|Z_1,Z_2}(a|b,c) = P(Y \\leq a|Z_1=b, Z_2=c).\n\nFor example:\n\n\nF_{Y|Z_1=10,Z_2=1}(a) is the CDF of wages among women with 10 years of experience.\n\nF_{Y|Z_1=10,Z_2=0}(a) is the CDF of wages among men with 10 years of experience.\n\n\n\n\n\n\n\n\n\n\n(a) Conditional CDFs\n\n\n\n\n\n\n\n\n\n(b) Conditional PDFs\n\n\n\n\n\n\nFigure 1.8: Wage distributions conditional on 10 years of experience and gender\n\n\nClearly the random variable Y and the random vector (Z_1, Z_2) are dependent.\nMore generally, we can condition on the event that a k-variate random vector \\boldsymbol Z = (Z_1, \\ldots, Z_k)' takes the value \\{\\boldsymbol Z = \\boldsymbol b\\}, i.e., \\{Z_1 = b_1, \\ldots, Z_k = b_k\\}. The conditional CDF of Y given \\{\\boldsymbol Z = \\boldsymbol b\\} is \n  F_{Y|\\boldsymbol Z = \\boldsymbol b}(a) = F_{Y|Z_1 = b_1, \\ldots, Z_k = b_k}(a).\n\nThe variable of interest, Y, can also be discrete. Then, any conditional CDF of Y is also discrete. Below is the conditional CDF of education given the married dummy variable:\n\n\nF_{Y|Z=0}(a) is the CDF of education among unmarried individuals.\n\nF_{Y|Z=1}(a) is the CDF of education among married individuals.\n\n\n\n\n\n\nFigure 1.9: Conditional CDFs of education given married\n\n\nThe conditional PMFs \\pi_{Y|Z=0}(a) = P(Y = a | Z=0) and \\pi_{Y|Z=1}(a)= P(Y = a | Z=1) indicate the jump heights of F_{Y|Z=0}(a) and F_{Y|Z=1}(a) at a.\n\n\n\n\n\n\n\n\n\n(a) Education given unmarried\n\n\n\n\n\n\n\n\n\n(b) Education given married\n\n\n\n\n\n\nFigure 1.10: Conditional PMFs of education for unmarried (left) and married (right) individuals\n\n\nClearly, education and married are dependent random variables. For example, \\pi_{Y|Z=0}(12) &gt; \\pi_{Y|Z=1}(12) and \\pi_{Y|Z=0}(18) &lt; \\pi_{Y|Z=1}(18).\nIn contrast, consider Y= coin flip and Z= married dummy variable. The CDF of a coin flip should be the same for married or unmarried individuals:\n\n\n\n\n\n\n\n\n\n(a) Coin flip given unmarried\n\n\n\n\n\n\n\n\n\n(b) Coin flip given married\n\n\n\n\n\n\nFigure 1.11: Conditional CDFs of a coin flip for unmarried (left) and married (right) individuals\n\n\nBecause \n  F_Y(a) = F_{Y|Z=0}(a) = F_{Y|Z=1}(a) \\quad \\text{for all} \\ a\n we say that Y and Z are independent random variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "part1_distribution.html#independence-of-random-variables",
    "href": "part1_distribution.html#independence-of-random-variables",
    "title": "\n1  Probability Distribution\n",
    "section": "\n1.9 Independence of Random Variables",
    "text": "1.9 Independence of Random Variables\nIn the previous section, we saw that the distribution of a coin flip remains the same regardless of a person’s marital status, illustrating the concept of independence. Let’s now formalize this important concept.\n\nIndependence\nY and Z are independent if and only if \n  F_{Y|Z=b}(a) = F_{Y}(a) \\quad \\text{for all} \\ a \\ \\text{and} \\ b.\n\n\n\nNote that if F_{Y|Z=b}(a) = F_{Y}(a) for all b, then automatically F_{Z|Y=a}(b) = F_{Z}(b) for all a. Due to this symmetry we can equivalently define independence through the property F_{Z|Y=a}(b) = F_{Z}(b).\n\nTechnical Note: More rigorously, the independence condition should state “for almost every b” rather than “for all b”. This means the condition must hold for every b in the support of Z, apart from a set of values that has probability 0 under Z. Put differently, the condition must hold for all b-values that Z can actually take, with exceptions allowed only on a set whose probability is 0. Think of it as “for all practical purposes”. For instance, we only need independence to hold for non-negative wages. We don’t need to check independence for negative wages since they can’t occur.\n\nFor discrete random variables, independence can be expressed using PMFs: Y and Z are independent if and only if \\pi_{Y|Z=b}(a) = \\pi_Y(a) for all a in the support of Y and all b in the support of Z. Similarly, for continuous random variables, independence means the conditional PDF factorizes f_{Y|Z=b}(a) = f_Y(a).\nThe definition naturally generalizes to Z_1, Z_2, Z_3. They are mutually independent if, for each i \\in \\{1,2,3\\}, the conditional distribution of Z_i given the other two equals its marginal distribution. In CDF form, this means:\n\nF_{Z_1|Z_2=b_2, Z_3=b_3}(a) = F_{Z_1}(a)\nF_{Z_2|Z_1=b_1, Z_3=b_3}(a) = F_{Z_2}(a)\nF_{Z_3|Z_1=b_1, Z_2=b_2}(a) = F_{Z_3}(a)\n\nfor all a and for all (b_1, b_2, b_3). Here, we need all three conditions.\n\nMutual Independence\nThe random variables Z_1, \\ldots, Z_n are mutually independent if and only if, for each i = 1,\\dots,n, \n  F_{Z_i | Z_1=b_1,\\ldots,Z_{i-1}=b_{i-1},\\,Z_{i+1}=b_{i+1},\\ldots,Z_n=b_n}(a)\n=\nF_{Z_i}(a)\n for all a and all (b_1, \\ldots, b_n).\n\n\nAn equivalent viewpoint uses the joint CDF of the vector \\boldsymbol Z = (Z_1, \\ldots, Z_n)', which is defined as: \n  F_{\\boldsymbol Z}(\\boldsymbol a) = F_{Z_1, \\ldots, Z_n}(a_1, \\ldots, a_n) = P(Z_1 \\leq a_1, \\ldots, Z_n \\leq a_n) = P(\\boldsymbol Z \\leq \\boldsymbol a),\n where \nP(Z_1 \\leq a_1, \\ldots, Z_n \\leq a_n) = P(\\{Z_1 \\leq a_1\\} \\cap \\ldots \\cap \\{Z_n \\leq a_n\\}).\n\nThen Z_1, \\ldots, Z_n are mutually independent if and only if the joint CDF is the product of the marginal CDFs: \n  F_{\\boldsymbol Z}(\\boldsymbol a) = F_{Z_1}(a_1) \\cdots F_{Z_n}(a_n) \\quad \\text{for all} \\ a_1, \\ldots, a_n.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "part1_distribution.html#independent-and-identically-distributed",
    "href": "part1_distribution.html#independent-and-identically-distributed",
    "title": "\n1  Probability Distribution\n",
    "section": "\n1.10 Independent and Identically Distributed",
    "text": "1.10 Independent and Identically Distributed\nAn important concept in statistics is that of an independent and identically distributed (i.i.d.) sample. This arises naturally when we consider multiple random variables that share the same distribution and do not influence each other.\n\ni.i.d. Sample / Random Sample\nA collection of random variables Y_1, \\dots, Y_n is i.i.d. (independent and identically distributed) if:\n\nThey are mutually independent: for each i = 1,\\dots,n, \n  F_{Y_i | Y_1=b_1, \\ldots, Y_{i-1}=b_{i-1}, Y_{i+1}=b_{i+1}, \\ldots, Y_n = b_n}(a) = F_{Y_i}(a)\n for all a and all (b_1, \\ldots, b_n).\nThey have the same distribution function: F_{Y_i}(a) = F(a) for all i=1,\\ldots,n and all a.\n\n\n\nFor example, consider n coin flips, where each Y_i represents the outcome of the i-th flip (with Y_i=1 for heads and Y_i=0 for tails). If the coin is fair and the flips are performed independently, then Y_1, \\ldots, Y_n form an i.i.d. sample with\n\nF(a) = F_{Y_i}(a) = \\begin{cases} 0 & a &lt; 0 \\\\ 0.5 & 0 \\leq a &lt; 1 \\\\ 1 & a \\geq 1 \\end{cases}\n\\qquad \\text{for all} \\ i=1, \\ldots, n.\n\nSimilarly, if we randomly select n individuals from a large population and measure their wages, the resulting measurements Y_1, \\ldots, Y_n can be treated as an i.i.d. sample. Each Y_i follows the same distribution (the wage distribution in the population), and knowledge of one person’s wage doesn’t affect the distribution of another’s. The function F is called the population distribution or the data-generating process (DGP).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "part1_distribution.html#independence-of-random-vectors",
    "href": "part1_distribution.html#independence-of-random-vectors",
    "title": "\n1  Probability Distribution\n",
    "section": "\n1.11 Independence of Random Vectors",
    "text": "1.11 Independence of Random Vectors\nOften in practice, we work with multiple variables recorded for different individuals or time points. For example, consider two random vectors: \n  \\boldsymbol{X}_1 = (X_{11}, \\ldots, X_{1k})',\n  \\quad\n  \\boldsymbol{X}_2 = (X_{21}, \\ldots, X_{2k})'.\n\nThe conditional distribution function of \\boldsymbol{X}_1 given that \\boldsymbol{X}_2 takes the value \\boldsymbol{b}=(b_1,\\ldots,b_k)' is \n  F_{\\boldsymbol{X}_1 | \\boldsymbol{X}_2 = \\boldsymbol{b}}(\\boldsymbol{a})\n  =\n  P(\\boldsymbol{X}_1 \\le \\boldsymbol{a}|\\boldsymbol{X}_2 = \\boldsymbol{b}),\n where the vector inequality \\boldsymbol{X}_1 \\le \\boldsymbol{a} represents the intersection of component-wise inequalities, i.e., \\{X_{11} \\le a_1\\} \\cap \\{X_{12} \\le a_2\\} \\cap \\cdots \\cap \\{X_{1k} \\le a_k\\}.\nFor instance, if \\boldsymbol{X}_1 and \\boldsymbol{X}_2 represent the survey answers of two different, randomly chosen people, then F_{\\boldsymbol{X}_2 | \\boldsymbol{X}_1=\\boldsymbol{b}}(\\boldsymbol{a}) describes the distribution of the second person’s answers, given that the first person’s answers are \\boldsymbol{b}.\nIf the two people are truly randomly selected and unrelated to one another, we would not expect \\boldsymbol{X}_2 to depend on whether \\boldsymbol{X}_1 equals \\boldsymbol{b} or some other value \\boldsymbol{c}. In other words, knowing \\boldsymbol X_1 provides no information that changes the distribution of \\boldsymbol X_2.\n\nIndependence of Random Vectors\nTwo random vectors \\boldsymbol{X}_1 and \\boldsymbol{X}_2 are independent if and only if \nF_{\\boldsymbol{X}_1 | \\boldsymbol{X}_2 = \\boldsymbol{b}}(\\boldsymbol{a})\n=\nF_{\\boldsymbol{X}_1}(\\boldsymbol{a})\n\\quad\n\\text{for all } \\boldsymbol{a}\n\\\n\\text{and} \\ \\boldsymbol{b}.\n\n\n\nThis definition extends naturally to mutual independence of n random vectors \\boldsymbol{X}_1,\\dots,\\boldsymbol{X}_n, where \\boldsymbol{X}_i = (X_{i1},\\dots,X_{ik})'. They are called mutually independent if, for each i = 1,\\dots,n, \n  F_{\\boldsymbol X_i| \\boldsymbol X_1=\\boldsymbol b_1, \\ldots, \\boldsymbol X_{i-1}=\\boldsymbol b_{i-1}, \\boldsymbol X_{i+1}=\\boldsymbol b_{i+1}, \\ldots, \\boldsymbol X_n = \\boldsymbol b_n}(\\boldsymbol a) = F_{\\boldsymbol X_i}(\\boldsymbol a)\n for all \\boldsymbol{a} and all (\\boldsymbol{b}_1,\\dots,\\boldsymbol{b}_n).\nHence, in an independent sample, what the i-th randomly chosen person answers does not depend on anyone else’s answers.\n\ni.i.d. Sample of Random Vectors\nThe concept of i.i.d. samples naturally extends to random vectors. A collection of random vectors \\boldsymbol{X}_1, \\dots, \\boldsymbol{X}_n is i.i.d. if they are mutually independent and have the same distribution function F. Formally, \n  F_{\\boldsymbol X_i| \\boldsymbol X_1=\\boldsymbol b_1, \\ldots, \\boldsymbol X_{i-1}=\\boldsymbol b_{i-1}, \\boldsymbol X_{i+1}=\\boldsymbol b_{i+1}, \\ldots, \\boldsymbol X_n = \\boldsymbol b_n}(\\boldsymbol a) = F(\\boldsymbol a)\n for all i=1, \\ldots, n, for all \\boldsymbol{a}, and all (\\boldsymbol{b}_1,\\dots,\\boldsymbol{b}_n).\nAn i.i.d. dataset (or random sample) is one where each multivariate observation not only comes from the same population distribution F but is independent of the others.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Distribution</span>"
    ]
  },
  {
    "objectID": "part2_expectation.html",
    "href": "part2_expectation.html",
    "title": "2  Expected Value",
    "section": "",
    "text": "2.1 Discrete Case\nAs previously defined, a discrete random variable Y is one that can take on a countable number of distinct values. The probability that Y takes a specific value a is given by the probability mass function (PMF) \\pi(a)=P(Y=a).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expected Value</span>"
    ]
  },
  {
    "objectID": "part2_expectation.html#discrete-case",
    "href": "part2_expectation.html#discrete-case",
    "title": "2  Expected Value",
    "section": "",
    "text": "2.1.1 Expectation\n\nExpected Value (Discrete Case)\nThe expectation or expected value of a discrete random variable Y with PMF \\pi(\\cdot) and support \\mathcal Y is defined as E[Y] = \\sum_{u \\in \\mathcal Y} u \\cdot \\pi(u). \\tag{2.1}\n\n\n\nThe expected value can be interpreted as the long-run average outcome of the random variable Y if we were to observe it repeatedly in independent experiments. For example, if we flip a fair coin many times, the proportion of heads will approach 0.5, which is the expected value of the coin toss random variable.\n\nExample: Binary Random Variable\nA binary or Bernoulli random variable Y takes on only two possible values: 0 and 1. The support is \\mathcal Y = \\{0,1\\}, and the PMF is \\pi(1) = p and \\pi(0) = 1 - p for some p \\in (0,1). The expected value of Y is:\nE[Y] = 0\\cdot\\pi(0) + 1\\cdot\\pi(1) = 0 \\cdot(1-p) + 1 \\cdot p = p.\nFor the variable coin, the probability of heads is p=0.5 and the expected value is E[Y] = p = 0.5.\n\n\nExample: Education Variable\nUsing the variable education with its PMF values introduced previously, we can calculate the expected value:\n\\begin{align*}\nE[Y] &= 4\\cdot\\pi(4) + 10\\cdot\\pi(10) + 12\\cdot \\pi(12) + 13\\cdot\\pi(13) \\\\\n&\\quad + 14\\cdot\\pi(14) + 16\\cdot \\pi(16) + 18 \\cdot \\pi(18) + 21 \\cdot \\pi(21) \\\\\n&= 0.032 + 0.55 + 4.716 + 1.027 + 2.03 + 1.248 + 3.924 + 0.504 \\\\\n&= 14.031\n\\end{align*}\nSo, the expected value of education is 14.031 years, which corresponds roughly to the completion of short-cycle tertiary education (ISCED level 5).\n\n\n\n2.1.2 Conditional Expectation\nPreviously, we introduced conditional probability distributions, which describe the distribution of a random variable given that another random variable takes a specific value. Building on this foundation, we can define the conditional expectation, which measures the expected value of a random variable when we have information about another random variable.\n\nConditional Expectation Given a Fixed Value\nFor a discrete random variable Y with conditional PMF \\pi_{Y|Z=b}(a), the conditional expectation of Y given Z=b is defined as:\nE[Y|Z=b] = \\sum_{u \\in \\mathcal{Y}} u \\cdot \\pi_{Y|Z=b}(u)\n\n\n\nThis formula closely resembles the unconditional expectation, but uses the conditional PMF instead of the marginal PMF. The conditional expectation E[Y|Z=b] can be interpreted as the average value of Y we expect to observe, given that we know Z has taken the value b.\n\nExample: Education Given Marital Status\nLet’s examine the conditional PMFs of education given marital status studied previously.\nFor unmarried individuals (Z=0): \\pi_{Y|Z=0}(a) =\n\\begin{cases}\n0.01 & \\text{if } a = 4 \\\\\n0.07 & \\text{if } a = 10 \\\\\n0.43 & \\text{if } a = 12 \\\\\n0.09 & \\text{if } a = 13 \\\\\n0.10 & \\text{if } a = 14 \\\\\n0.09 & \\text{if } a = 16 \\\\\n0.19 & \\text{if } a = 18 \\\\\n0.02 & \\text{if } a = 21 \\\\\n0 & \\text{otherwise}\n\\end{cases}\nFor married individuals (Z=1): \\pi_{Y|Z=1}(a) =\n\\begin{cases}\n0.01 & \\text{if } a = 4 \\\\\n0.03 & \\text{if } a = 10 \\\\\n0.38 & \\text{if } a = 12 \\\\\n0.07 & \\text{if } a = 13 \\\\\n0.17 & \\text{if } a = 14 \\\\\n0.06 & \\text{if } a = 16 \\\\\n0.25 & \\text{if } a = 18 \\\\\n0.03 & \\text{if } a = 21 \\\\\n0 & \\text{otherwise}\n\\end{cases}\nThe conditional expectation of education for unmarried individuals is:\n\\begin{align*}\nE[Y|Z=0] &= 4 \\cdot 0.01 + 10 \\cdot 0.07 + 12 \\cdot 0.43 + 13 \\cdot 0.09 \\\\\n&\\quad + 14 \\cdot 0.10 + 16 \\cdot 0.09 + 18 \\cdot 0.19 + 21 \\cdot 0.02 \\\\\n&= 13.75\n\\end{align*}\nThe conditional expectation of education for married individuals is:\n\\begin{align*}\n  E[Y|Z=1] &= 4 \\cdot 0.01 + 10 \\cdot 0.03 + 12 \\cdot 0.38 + 13 \\cdot 0.07 \\\\\n  &\\quad + 14 \\cdot 0.17 + 16 \\cdot 0.06 + 18 \\cdot 0.25 + 21 \\cdot 0.03 \\\\\n  &= 14.28\n\\end{align*}\nWe observe that the expected education level is higher for married individuals (14.28 years) compared to unmarried individuals (13.75 years), which suggests a dependence between marital status and education.\n\n\n\n2.1.3 Conditional Expectation Function (CEF)\nSo far, we have used E[Y|Z=b] to denote the conditional expectation of Y given a specific value b of Z. This is a fixed number for each value of b. A related concept is the Conditional Expectation Function, denoted as E[Y|Z] without specifying a particular value for Z.\n\nConditional Expectation Function (CEF)\nThe conditional expectation function E[Y|Z] represents a random variable that depends on the random outcome of Z. It is a function that maps each possible value of Z to the corresponding conditional expectation:\nE[Y|Z] = m(Z) \\quad \\text{where} \\quad m(b) = E[Y|Z=b]\nHere, m(\\cdot) is the function that represents the CEF, mapping each possible value of Z to the corresponding conditional expectation.\n\n\n\nThe CEF is random precisely because it is a function of the random variable Z. Before we observe the value of Z, we cannot determine the value of E[Y|Z]. Once we observe Z, the CEF gives us the expected value of Y corresponding to that specific observation. This makes E[Y|Z] a random variable whose value depends on the random outcome of Z. In contrast, E[Y|Z=b] is a deterministic scalar non-random value.\nFor our marital status example, the CEF is:\nE[Y|Z] = m(Z) = \\begin{cases}\n13.75 & \\text{if } Z = 0 \\text{ (unmarried)} \\\\\n14.28 & \\text{if } Z = 1 \\text{ (married)}\n\\end{cases}\nIn our population, the marginal PMF of married is \n    \\pi_Z(a) = \\begin{cases}\n0.4698 & \\text{if } a = 0  \\ (\\text{unmarried})\\\\\n0.5302 & \\text{if } a = 1 \\ (\\text{married}) \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\nUsing these values the PMF of E[Y|Z] is: \n\\pi_{E[Y|Z]}(a) = P(E[Y|Z] = a) = \\begin{cases}\n0.4698 & \\text{if } a = 13.75  \\\\\n0.5302 & \\text{if } a = 14.28 \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\n\n\n2.1.4 Law of Iterated Expectations (LIE)\n\nLaw of Iterated Expectations\nFor two random variables Y and Z: E[Y] = E[E[Y|Z]]\n\n\n\nThis elegant equation states that the expected value of Y can be found by first calculating the conditional expectation of Y given Z (which gives us the random variable E[Y|Z]), and then taking the expected value of this random variable. In other words, we are taking the expectation of the conditional expectation.\nThe Law of Iterated Expectations is a fundamental tool in econometrics with numerous applications. It is particularly important for understanding the properties of estimators in the presence of conditioning variables like in regression analysis.\nTo understand why this law holds, let’s consider an intuitive argument based on the law of total probability. For discrete random variables, the law of total probability tells us that we can find the overall probability of an event Y=a by considering all possible scenarios Z=b that could lead to that event. More precisely, P(Y=a) equals the weighted sum of conditional probabilities P(Y=a|Z=b) across all possible values b of Z, where each conditional probability is weighted by P(Z=b): \\pi_Y(a) = \\sum_{b \\in \\mathcal{Z}} \\pi_{Y|Z=b}(a) \\cdot \\pi_Z(b)\nThe LIE follows a similar logic. We can think of the overall expectation of Y as a weighted average of conditional expectations E[Y|Z=b] across all possible values of Z, with each conditional expectation weighted by the probability of the corresponding Z value:\nE[Y] = \\sum_{u \\in \\mathcal{Z}} E[Y|Z=u] \\cdot \\pi_Z(u)\nThe right hand side is precisely what E[E[Y|Z]] means: take the conditional expectation function E[Y|Z] and average it over all possible values u\\in \\mathcal Z of Z, where \\mathcal Z is the support of Z.\nFor our education and marital status example, the LIE gives us:\n\\begin{align*}\n  E[Y] &= E[E[Y|Z]] \\\\\n  &= E[Y|Z=0] \\cdot \\pi_Z(0) + E[Y|Z=1] \\cdot \\pi_Z(1) \\\\\n  &= 13.75 \\cdot 0.4698 + 14.28 \\cdot 0.5302 \\\\\n  &= 6.460 + 7.571 \\\\\n  &= 14.031\n\\end{align*}\nThis matches exactly with our directly calculated expected value of 14.031 years from the marginal PMF.\n\n\n2.1.5 Conditioning Theorem (CT)\n\nConditioning Theorem / Factorization Property\nFor two random variables Y and Z:\nE[ZY|Z] = Z \\cdot E[Y|Z]\n\n\n\nTo see this, let’s first consider the case for a specific value Z=b:\nE[bY|Z=b] = \\sum_{u \\in \\mathcal{Y}} b \\cdot u \\cdot \\pi_{Y|Z=b}(u) = b \\sum_{u \\in \\mathcal{Y}} u \\cdot \\pi_{Y|Z=b}(u) = b \\cdot E[Y|Z=b]\nWhen we consider this factorization across all possible values of Z rather than a fixed value b, we get the general form of the Conditioning Theorem: E[ZY|Z] = Z \\cdot E[Y|Z].\nThe conditioning theorem states that we can factor out the conditioning variable Z from the conditional expectation. The intuition is that when we condition on Z, we’re essentially treating it as if we already know its value, so it behaves like a constant within the conditional expectation. Since summation is linear and constants can be factored out, Z can be factored out of E[ZY|Z].\nThis theorem is particularly useful in econometric derivations, especially when working with regression models.\nFor example, in our marital status context, if we want to compute E[Z Y|Z] (the conditional expectation of education multiplied by marital status, given marital status), we get:\nE[Z Y|Z] = Z \\cdot E[Y|Z] = \\begin{cases}\n0 \\cdot 13.75 = 0 & \\text{if } Z = 0 \\text{ (unmarried)} \\\\\n1 \\cdot 14.28 = 14.28 & \\text{if } Z = 1 \\text{ (married)}\n\\end{cases}\nI’ve evaluated your draft for the “Continuous Case” subsection, and it’s a good start. Here’s my suggested improved version with better organization, more detailed explanations, and enhanced examples:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expected Value</span>"
    ]
  },
  {
    "objectID": "part2_expectation.html#continuous-case",
    "href": "part2_expectation.html#continuous-case",
    "title": "2  Expected Value",
    "section": "2.2 Continuous Case",
    "text": "2.2 Continuous Case\nNow, let’s extend our discussion of expected values to continuous random variables, which are characterized by probability density functions (PDFs) rather than probability mass functions (PMFs).\n\nExpected Value (Continuous Case)\nThe expectation or expected value of a continuous random variable Y with PDF f_Y(u) and support \\mathcal{Y} is defined as E[Y] = \\int_{\\mathcal{Y}} u f_Y(u) \\, du = \\int_{-\\infty}^{\\infty} u f_Y(u) \\, du. \\tag{2.2}\n\n\n\nIntuitively, this integral calculates a weighted average of all possible values of Y, where the weight of each value is given by its density. This is analogous to the discrete case, where we computed a weighted sum. The key difference is that continuous random variables have infinitely many possible values within their support, requiring integration rather than summation.\n\n2.2.1 Conditional Expectation for Continuous Variables\nFor continuous random variables, the conditional expectation given that Z=b is defined similarly:\n\nConditional Expectation (Continuous Case)\nFor a continuous random variable Y with conditional PDF f_{Y|Z=b}(u), the conditional expectation of Y given Z=b is:\nE[Y|Z=b] = \\int_{-\\infty}^{\\infty} u f_{Y|Z=b}(u) \\, du\n\n\n\nThe same principles of conditional expectation functions (CEF) that we discussed for discrete variables apply here as well. The CEF E[Y|Z] = m(Z) is a random variable that depends on the random outcome of Z, where m(b) = E[Y|Z=b] is the function mapping each possible value of Z to the corresponding conditional expectation.\n\n\n2.2.2 Examples with Continuous Random Variables\n\nExample 1: Uniform Distribution\nA random variable Y follows a uniform distribution on the interval [0,1] if its PDF is constant across this interval:\nf(u) = \\begin{cases} 1 & \\text{if} \\ u \\in[0,1], \\\\ 0 & \\text{otherwise.} \\end{cases}\nThe expected value is calculated as:\nE[Y] = \\int_{-\\infty}^\\infty u f(u) \\, du = \\int_{0}^1 u \\cdot 1 \\, du = \\int_{0}^1 u \\, du = \\left. \\frac{u^2}{2} \\right|_{0}^1 = \\frac{1}{2}\nSo the expected value of a uniform random variable on [0,1] is exactly \\frac{1}{2}, the midpoint of the interval. More generally, for a uniform distribution on [a,b], the expected value is \\frac{a+b}{2}.\n\n\nExample 2: Wage Distribution\nLet’s return to our wage variable from Part 1. Suppose the wage distribution in our population has the following PDF:\nf(u) = \\begin{cases}\n\\frac{1}{20}e^{-u/20} & \\text{if} \\ u \\geq 0, \\\\\n0 & \\text{otherwise,}\n\\end{cases}\nThis represents an exponential distribution with parameter \\lambda = 1/20. The expected value is:\nE[Y] = \\int_{0}^{\\infty} u \\cdot \\frac{1}{20}e^{-u/20} \\, du = \\frac{1}{20} \\int_{0}^{\\infty} u \\cdot e^{-u/20} \\, du\nUsing integration by parts or leveraging the known mean of an exponential distribution (1/\\lambda), we get E[Y] = 20 EUR per hour.\n\n\nExample 3: Wage Given Education\nFrom Figure 1.7 (b), we saw that the conditional distribution of wage given education varies substantially:\n\n\n\n\n\n\n\n\nConditional PDFs of wage given education\n\n\n\n\n\n\nFigure 2.1\n\n\n\nThese different distributions lead to different conditional expectations:\n\nE[Y|Z=12] = 14.3 EUR/hour\nE[Y|Z=14] = 17.8 EUR/hour\nE[Y|Z=18] = 27.0 EUR/hour\n\nThe CEF plot is given below:\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: CEF of wage given education\n\n\n\nThe increasing conditional expectations reflect the positive relationship between education and wages. This dependency confirms that education and wages are not independent random variables.\n\n\nExample 4: Wage Given Experience\nNow consider the relationship between wage (Y) and years of experience (Z). Labor economics theory suggests that wages typically increase with experience but at a diminishing rate. Suppose empirical analysis reveals that the conditional expectation of wage given experience level b follows a quadratic form:\nm(b) = E[Y|Z=b] = 19 + 0.5b - 0.013b^2\nThis functional form captures the initial increase in wages with experience (positive linear term 0.5b) and the diminishing returns over time (negative quadratic term -0.013b^2).\nFor some specific values:\n\nm(5) = E[Y|Z=5] = 21.2 EUR/hour\nm(10) = E[Y|Z=10] = 22.7 EUR/hour\nm(20) = E[Y|Z=20] = 23.8 EUR/hour\nm(30) = E[Y|Z=30] = 22.3 EUR/hour\n\nThe conditional expectation function is maximized at the point where its derivative equals zero:\n\\frac{d}{db}m(b) = 0.5 - 0.026b = 0 \\implies b = \\frac{0.5}{0.026} \\approx 19.2\nThis suggests that, on average, wages peak at around 19.2 years of experience in this population.\nAs a function of the random variable Z, the CEF is:\nE[Y|Z] = m(Z) = 19 + 0.5Z - 0.013Z^2\nThis is itself a random variable because its value depends on the random outcome of Z.\n\n\n\n\n\n\nFigure 2.3: CEF of wage given experience",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expected Value</span>"
    ]
  },
  {
    "objectID": "part2_expectation.html#general-case",
    "href": "part2_expectation.html#general-case",
    "title": "2  Expected Value",
    "section": "2.3 General Case",
    "text": "2.3 General Case\nWe can define the expected value of a random variable Y in a unified way that applies to both discrete and continuous cases using its CDF F_Y(u).\n\nExpected Value (General Definition)\nE[Y] = \\int_{-\\infty}^\\infty u \\, \\text{d}F_Y(u) \\tag{2.3}\n\n\n\nThis formula uses the Riemann-Stieltjes integral, which generalizes the familiar Riemann integral. To understand this, recall that the standard Riemann integral \\int_a^b g(x) \\, dx is defined as: \\int_a^b g(x) \\, dx = \\lim_{n \\to \\infty} \\sum_{i=1}^{n} g(x_i^*) \\Delta x_i where [a,b] is partitioned into n subintervals [x_{i-1}, x_i] of width \\Delta x_i = x_i - x_{i-1}, and x_i^* is any point in the i-th subinterval. The limit is taken as the maximum width of all subintervals approaches zero.\nIn contrast, the Riemann-Stieltjes integral \\int_a^b g(x) \\, dh(x) is defined as: \\int_a^b g(x) \\, dh(x) = \\lim_{n \\to \\infty} \\sum_{i=1}^{n} g(x_i^*) \\Delta h_i where \\Delta h_i = h(x_i) - h(x_{i-1}) represents the increment in the function h over the i-th subinterval.\nIntuitively, while the standard Riemann integral weighs the function values by increments in the x-axis, the Riemann-Stieltjes integral weighs them by increments in another function, allowing us to seamlessly handle both continuous distributions (where we integrate against a smooth CDF) and discrete distributions (where the integrator function has jumps).\nFor infinite intervals, as in \\int_{-\\infty}^{\\infty} u \\, dF_Y(u), the integral is defined as: \\int_{-\\infty}^{\\infty} u \\, dF_Y(u) = \\lim_{a \\to -\\infty, b \\to \\infty} \\int_a^b u \\, dF_Y(u)\n\n2.3.1 Special Case: Continuous Random Variables\nFor a continuous random variable Y with PDF f_Y(u), the CDF F_Y(u) is differentiable with \\frac{dF_Y(u)}{du}=f_Y(u)\nHence: \\text{d}F_Y(u) = f_Y(u) \\, \\text{d}u\nSubstituting this into our unified definition: E[Y] = \\int_{-\\infty}^\\infty u \\, \\text{d}F_Y(u) = \\int_{-\\infty}^\\infty u \\cdot f_Y(u) \\, \\text{d}u\nThis recovers the standard definition for continuous random variables we saw earlier.\n\n\n2.3.2 Special Case: Discrete Random Variables\nFor a discrete random variable, the CDF F_Y(u) has jumps at each point u \\in \\mathcal{Y} where Y can take values. The size of each jump is: \\Delta F_Y(u) = F_Y(u) - F_Y(u^-) = P(Y=u) = \\pi_Y(u) where F_Y(u^-) = \\lim_{\\varepsilon \\to 0, \\varepsilon &gt; 0} F_Y(u - \\varepsilon) is the left limit of F_Y at u.\nFor discrete variables, the Riemann-Stieltjes integral simplifies to a sum over these jumps: E[Y] = \\int_{-\\infty}^\\infty u \\, \\text{d}F_Y(u) = \\sum_{u \\in \\mathcal{Y}} u \\cdot \\Delta F_Y(u) = \\sum_{u \\in \\mathcal{Y}} u \\cdot \\pi_Y(u)\nThis matches our earlier definition for discrete random variables.\n\n\n2.3.3 Why the General Case Matters\nThe unified approach to expected values offers several important advantages:\n\nConceptual and practical simplicity: Using the CDF as the foundation emphasizes that distinguishing between discrete and continuous random variables isn’t critically important for defining expectations. In econometric practice, expectations are computed without needing to categorize the distribution first, as the same principles apply regardless of distribution type.\nHandling mixed and exotic distributions: Many real-world variables have distributions that don’t fit neatly into pure categories—like wages with both continuous values and “spikes” at round numbers, or insurance claims that are zero with positive probability but continuous for positive values. The general definition also accommodates more exotic theoretical cases like the Cantor distribution.\nUnified theoretical development: For developing econometric theory, having a single definition simplifies proofs and ensures results apply broadly without requiring separate cases for different distribution types, allowing us to focus on understanding the properties and relationships between random variables.\n\n\n\n2.3.4 General Definition of Conditional Expectation\nJust as we can define the expected value in a unified way, we can also define conditional expectation in a general form that applies to all types of random variables.\n\nConditional Expectation (General Definition)\nFor random variables Y and Z, the conditional expectation of Y given Z=b is defined as: E[Y|Z=b] = \\int_{-\\infty}^{\\infty} u \\, \\text{d}F_{Y|Z=b}(u) where F_{Y|Z=b}(u) is the conditional CDF of Y given Z=b.\nSimilarly, for any random vector \\boldsymbol Z = (Z_1, \\ldots, Z_k)', the conditional expectation of Y given \\boldsymbol Z = \\boldsymbol b is: E[Y|\\boldsymbol Z=\\boldsymbol b] = \\int_{-\\infty}^{\\infty} u \\, \\text{d}F_{Y|\\boldsymbol Z=\\boldsymbol b}(u)\n\n\n\nThis definition uses the Riemann-Stieltjes integral with respect to the conditional CDF. For continuous random variables with conditional PDF f_{Y|Z=b}(u), this becomes: E[Y|Z=b] = \\int_{-\\infty}^{\\infty} u \\cdot f_{Y|Z=b}(u) \\, \\text{d}u\nFor discrete random variables with conditional PMF \\pi_{Y|Z=b}(u), it simplifies to: E[Y|Z=b] = \\sum_{u \\in \\mathcal{Y}} u \\cdot \\pi_{Y|Z=b}(u)\nThe conditional expectation function (CEF) is then defined as: E[Y|Z] = m(Z) where m(b) = E[Y|Z=b] for each possible value b that Z can take. This makes E[Y|Z] a random variable whose value depends on the random outcome of Z.\nSimilarly, if \\boldsymbol Z is a vector, we have: E[Y|\\boldsymbol Z] = m(\\boldsymbol Z) where m(\\boldsymbol b) = E[Y|\\boldsymbol Z = \\boldsymbol b].\nThis can also be extended to conditioning on a n\\times k matrix of random variables \\boldsymbol X (e.g., a regressor matrix), which gives E[Y|\\boldsymbol X]. This extension is particularly important in econometrics for regression analysis.\n\n\n2.3.5 Conditional Expectation and Independence\nWhen two random variables Y and Z are independent, the conditional distributions simplify considerably. As we saw in the first section, independence means that the conditional distribution of Y given Z=b is the same as the marginal distribution of Y. This fundamental property has important implications for conditional expectations.\n\nConditional Expectation and Independence\nIf random variables Y and Z are independent, then:\nE[Y|Z=b] = E[Y] \\quad \\text{for all } b\nAnd consequently:\nE[Y|Z] = E[Y]\n\n\n\nIn other words, when Y and Z are independent, knowing the value of Z provides no information about the expected value of Y. The conditional expectation equals the unconditional expectation for every possible value of Z.\nTo understand why this holds, recall that for independent random variables, the conditional CDF equals the marginal CDF. Using the general definition of conditional expectation:\nE[Y|Z=b] = \\int_{-\\infty}^{\\infty} u \\, \\text{d}F_{Y|Z=b}(u) = \\int_{-\\infty}^{\\infty} u \\, \\text{d}F_Y(u) = E[Y]\nThe middle equality holds because F_{Y|Z=b}(u) = F_Y(u) for all u when Y and Z are independent.\nThis means that the conditional expectation function E[Y|Z] = m(Z) reduces to a constant function:\nm(b) = E[Y] \\quad \\text{for all } b\nFor example, recall our coin flip example from Part 1, where we noted that the distribution of a coin toss remains the same regardless of whether a person is married or unmarried:\nF_{Y|Z=0}(a) = F_{Y|Z=1}(a) = F_Y(a) \\quad \\text{for all } a\nwhere Y represents the coin outcome and Z is the marital status. In this case, E[Y|Z=0] = E[Y|Z=1] = E[Y] = 0.5, reflecting that the expected outcome of a fair coin toss is 0.5 regardless of the marital status of the person tossing it. Hence, E[Y|Z] = E[Y] for this example.\n\n\n2.3.6 Expected Value of Functions\nOften, we are interested not just in the expected value of a random variable Y itself, but in the expected value of some function of Y, such as Y^2 or \\log(Y).\n\nExpected Value of a Function\nFor any function g(\\cdot), the expected value of g(Y) is: E[g(Y)] = \\int_{-\\infty}^{\\infty} g(u) \\, \\text{d}F_Y(u)\nFor conditional expectations: E[g(Y)|Z=b] = \\int_{-\\infty}^{\\infty} g(u) \\, \\text{d}F_{Y|Z=b}(u)\n\n\n\nThe conditional expectation function is E[g(Y)|Z]=m(Z), where m(b) = E[g(Y)|Z=b].\nAs discussed above for the different cases, \\text{d}F_Y(u) can be replaced by the PMF or the PDF: \n  \\int_{-\\infty}^\\infty g(u) \\, \\text{d}F_Y(u) = \\begin{cases}\n      \\sum_{u \\in \\mathcal Y} g(u) \\pi_Y(u) & \\text{if} \\ Y \\ \\text{is discrete,} \\\\\n      \\int_{-\\infty}^\\infty g(u) f_Y(u)\\,\\text{d}u & \\text{if} \\ Y \\ \\text{is continuous.} \\end{cases}\n\n\nExample: Transformation of a Binary Variable\nFor instance, if we take the coin variable Y and consider the transformed random variable \\log(Y+1), the expected value is: \\begin{align*}\n  E[\\log(Y+1)]\n  &= \\log(1) \\cdot \\pi_Y(0) + \\log(2) \\cdot \\pi_Y(1) \\\\\n  &= \\log(1) \\cdot \\frac{1}{2} + \\log(2) \\cdot \\frac{1}{2} \\\\\n  &= \\frac{\\log(2)}{2} \\approx 0.347\n\\end{align*}\nThis approach allows us to compute expectations of arbitrary functions of random variables.\n\n\n\n2.3.7 Moments and Related Measures\nWe can define various moments of a random variable and functions of these moments using our general expectation framework:\n\nMoments and Related Measures\n\nr-th moment of Y: E[Y^r] = \\int_{-\\infty}^\\infty u^r \\, \\text{d}F_Y(u)\nr-th central moment: E[(Y-E[Y])^r] = \\int_{-\\infty}^\\infty (u - E[Y])^r \\, \\text{d}F_Y(u)\nVariance (2nd central moment): \\text{Var}(Y) = E[(Y-E[Y])^2] = \\int_{-\\infty}^\\infty (u - E[Y])^2 \\, \\text{d}F_Y(u)\nStandard deviation: \\text{sd}(Y) = \\sqrt{\\text{Var}(Y)}\nr-th standardized moment: E \\left[ \\left(\\frac{Y-E[Y]}{\\text{sd}(Y)}\\right)^r \\right] = \\int_{-\\infty}^\\infty \\left(\\frac{u-E[Y]}{\\text{sd}(Y)}\\right)^r \\, \\text{d}F_Y(u)\nSkewness (3rd standardized moment): \\text{ske}(Y) = E \\left[ \\left(\\frac{Y-E[Y]}{\\text{sd}(Y)}\\right)^3 \\right]\nKurtosis (4th standardized moment): \\text{kur}(Y) = E \\left[ \\left(\\frac{Y-E[Y]}{\\text{sd}(Y)}\\right)^4 \\right]\n\n\n\n\nSimilarly, conditional versions of these moments can be defined. For example:\n\nThe r-th conditional moment: E[Y^r|Z=b] = \\int_{-\\infty}^\\infty u^r \\, \\text{d}F_{Y|Z=b}(u)\nThe conditional variance: \\begin{align*}\n\\text{Var}(Y|Z=b)\n&= E[(Y-E[Y|Z=b])^2|Z=b] \\\\\n&= \\int_{-\\infty}^\\infty (u - E[Y|Z=b])^2 \\, \\text{d}F_{Y|Z=b}(u)\n\\end{align*}\nThe conditional variance function: \\text{Var}(Y|Z) = v(Z), \\quad \\text{where} \\ v(b) = \\text{Var}(Y|Z=b)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expected Value</span>"
    ]
  },
  {
    "objectID": "part2_expectation.html#properties-of-expectation",
    "href": "part2_expectation.html#properties-of-expectation",
    "title": "2  Expected Value",
    "section": "2.4 Properties of Expectation",
    "text": "2.4 Properties of Expectation\nThe general expected value operator has several important properties. Here we focus on three fundamental properties: linearity, the Law of Iterated Expectations, and the Conditioning Theorem.\n\n2.4.1 Linearity\n\nLinearity of Expectation\nFor any constants a, b \\in \\mathbb{R} and random variable Y: E[a+bY]=a+bE[Y]\nThe same property holds for conditional expectations: E[a+bY|Z] = a + bE[Y|Z]\n\n\n\nThis property tells us that the expectation of a linear transformation of a random variable equals the same linear transformation of the expectation.\nTo understand why this holds, consider the definition of expectation using the Riemann-Stieltjes integral: E[a+bY] = \\int_{-\\infty}^\\infty (a+bu) \\, \\text{d}F_Y(u)\nWe can separate this into two parts: \\int_{-\\infty}^\\infty (a+bu) \\, \\text{d}F_Y(u) = \\int_{-\\infty}^\\infty a \\, \\text{d}F_Y(u) + \\int_{-\\infty}^\\infty bu \\, \\text{d}F_Y(u)\nThe first integral equals a because \\int_{-\\infty}^\\infty \\text{d}F_Y(u) = 1 for any probability distribution. The second integral equals b\\cdot E[Y]. Combining these results gives us a + bE[Y].\n\n\n2.4.2 Law of Iterated Expectations (LIE)\n\nLaw of Iterated Expectations\nFor any random variables Y and Z: E[Y]=E[E[Y|Z]]\n\n\n\nThe LIE states that the expected value of Y can be found by first taking the conditional expectation of Y given Z, and then taking the expectation of this result over the distribution of Z.\nThis result relies on the law of total probability, which connects marginal and conditional distributions. For discrete random variables, this law states: \\pi_Y(u) = \\sum_{b \\in \\mathcal{Z}} \\pi_{Y|Z=b}(u) \\cdot \\pi_Z(b)\nFor continuous random variables with densities, the law takes the form: f_Y(u) = \\int_{-\\infty}^\\infty f_{Y|Z=b}(u) \\cdot f_Z(b) \\, db\nIn the general case using CDFs, the law of total probability is expressed as: \\text{d}F_Y(u) = \\int_{-\\infty}^\\infty \\text{d}F_{Y|Z=b}(u) \\, \\text{d}F_Z(b)\nWhen we evaluate E[E[Y|Z]], we are calculating: E[E[Y|Z]] = \\int_{-\\infty}^\\infty E[Y|Z=b] \\, \\text{d}F_Z(b)\nExpanding the inner conditional expectation: E[E[Y|Z]] = \\int_{-\\infty}^\\infty \\left(\\int_{-\\infty}^\\infty u \\, \\text{d}F_{Y|Z=b}(u)\\right) \\, \\text{d}F_Z(b)\nApplying the general form of the law of total probability, this double integral simplifies to: E[E[Y|Z]] = \\int_{-\\infty}^\\infty u \\, \\text{d}F_Y(u) = E[Y]\n\n\n2.4.3 Conditioning Theorem (CT)\n\nConditioning Theorem\nFor any random variables Y and Z, and any measurable function g(\\cdot): E[g(Z)Y|Z]=g(Z)E[Y|Z]\n\n\n\nThe Conditioning Theorem states that when we condition on Z, we can factor out any function of Z from the conditional expectation.\nTo see why this holds, consider a specific value Z=b. The conditional expectation becomes: E[g(Z)Y|Z=b] = E[g(b)Y|Z=b] = g(b)E[Y|Z=b]\nThe first equality follows because g(Z) = g(b) when Z=b. The second equality uses the linearity property of expectation, treating g(b) as a constant.\nSince this relationship holds for every possible value of Z, we have the general result: E[g(Z)Y|Z]=g(Z)E[Y|Z]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Expected Value</span>"
    ]
  },
  {
    "objectID": "part3_covariance.html",
    "href": "part3_covariance.html",
    "title": "3  Multiple Random Variables",
    "section": "",
    "text": "3.1 Expectations with Multiple Random Variables\nIn Parts 1 and 2, we focused on single random variables and their properties. Now we extend these concepts to scenarios involving multiple random variables, which is essential for analyzing relationships between variables in statistical modeling.\nRecall that for any univariate function g(\\cdot), the expected value of g(Y) and the conditional expectation given Z=b are: E[g(Y)] = \\int_{-\\infty}^\\infty g(u) \\ \\text{d}F_Y(u), \\quad E[g(Y)|Z=b] = \\int_{-\\infty}^\\infty g(u) \\ \\text{d}F_{Y|Z=b}(u)\nwhere F_Y(a) is the marginal CDF of Y and F_{Y|Z=b}(a) is the conditional CDF of Y given Z=b.\nFor functions of multiple random variables, we extend this approach using multivariate functions. For a bivariate function h(Y,Z), we can calculate the expected value using the Law of Iterated Expectations (LIE):",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Random Variables</span>"
    ]
  },
  {
    "objectID": "part3_covariance.html#expectations-with-multiple-random-variables",
    "href": "part3_covariance.html#expectations-with-multiple-random-variables",
    "title": "3  Multiple Random Variables",
    "section": "",
    "text": "Expected Value of a Bivariate Function\nFor a bivariate function h(Y,Z), the expected value can be calculated as: E[h(Y,Z)] = E[E[h(Y,Z)|Z]]\nThis double expectation can be expressed as: \\begin{align*}\nE[E[h(Y,Z)|Z]] &= \\int_{-\\infty}^\\infty E[h(Y,Z)|Z=b] \\ \\text{d}F_Z(b) \\\\\n&= \\int_{-\\infty}^\\infty \\left(\\int_{-\\infty}^\\infty h(u,b) \\ \\text{d}F_{Y|Z=b}(u)\\right) \\ \\text{d}F_Z(b)\n\\end{align*}\n\n\n\n3.1.1 Important Special Cases\n\nSum of Random Variables\nFor the sum of two random variables, h(Y,Z) = Y + Z, we have:\n\\begin{align*}\nE[Y+Z] &= E[E[Y+Z|Z]] \\\\\n&= E[E[Y|Z] + Z] \\quad \\text{(linearity of conditional expectation)} \\\\\n&= E[E[Y|Z]] + E[Z] \\quad \\text{(linearity of expectation)} \\\\\n&= E[Y] + E[Z] \\quad \\text{(by the LIE)}\n\\end{align*}\n\nLinearity of Expectation for Sums\nThe expected value of a sum equals the sum of the expected values: E[Y+Z] = E[Y] + E[Z]\nThis property holds regardless of whether Y and Z are independent or dependent.\n\n\n\n\n\nProduct of Random Variables\nFor the product of two random variables, h(Y,Z) = YZ, we have:\n\\begin{align*}\nE[YZ] &= E[E[YZ|Z]] \\\\\n&= E[Z \\cdot E[Y|Z]] \\quad \\text{(by the CT)}\n\\end{align*}\nWhen Y and Z are independent, E[Y|Z]=E[Y], so:\nE[YZ] = E[Z \\cdot E[Y]] = E[Z] \\cdot E[Y]\n\nExpected Value of a Product\n\nGeneral case: E[YZ] = E[Z \\cdot E[Y|Z]]\nIndependent case: If Y and Z are independent, then E[YZ] = E[Y] \\cdot E[Z]\n\n\n\n\nE[YZ] is also known as the first cross moment of Y and Z.\n\n\nExample: Product of Education and Wage\nConsider the random variables education (Z) and wage (Y) from our earlier examples. These variables are dependent, with E[Y|Z=b] following the pattern shown in the CEF plot from the previos section.\nIf E[Y|Z=b] = 2 + 1.2b (a simplified linear relationship), then:\n\\begin{align*}\nE[YZ] &= E[Z \\cdot E[Y|Z]] \\\\\n&= E[Z \\cdot (2 + 1.2Z)] \\\\\n&= E[2Z + 1.2Z^2] \\\\\n&= 2E[Z] + 1.2E[Z^2]\n\\end{align*}\nIf E[Z] = 14 years (mean education) and E[Z^2] = 210 (second moment of education), then: E[YZ] = 2 \\cdot 14 + 1.2 \\cdot 210 = 28 + 252 = 280\n\n\n\n3.1.2 Extending to Three or More Variables\nFor functions of three or more random variables, we can extend this approach by nesting conditional expectations. For h(X,Y,Z):\nE[h(X,Y,Z)] = E[E[E[h(X,Y,Z)|X,Y]|X]]\nThis formula allows us to decompose the expectation iteratively, conditioning on one variable at a time.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Random Variables</span>"
    ]
  },
  {
    "objectID": "part3_covariance.html#covariance-and-correlation",
    "href": "part3_covariance.html#covariance-and-correlation",
    "title": "3  Multiple Random Variables",
    "section": "3.2 Covariance and Correlation",
    "text": "3.2 Covariance and Correlation\nHaving explored expectations involving multiple random variables, we now introduce measures that quantify the relationship between random variables: covariance and correlation.\n\n3.2.1 Definition of Covariance\n\nCovariance\nThe covariance between two random variables Y and Z is defined as:\n\\text{Cov}(Y,Z) = E[(Y-E[Y])(Z-E[Z])]\nAn equivalent and often more practical definition is: \\text{Cov}(Y,Z) = E[YZ] - E[Y]E[Z]\n\n\n\nThe equivalence of these definitions can be shown by expanding the first expression: \\begin{align*}\n\\text{Cov}(Y,Z) &= E[(Y-E[Y])(Z-E[Z])] \\\\\n&= E[YZ - Y\\cdot E[Z] - Z \\cdot E[Y] + E[Y]E[Z]] \\\\\n&= E[YZ] - E[Y]E[Z] - E[Y]E[Z] + E[Y]E[Z] \\\\\n&= E[YZ] - E[Y]E[Z]\n\\end{align*}\n\n\n3.2.2 Interpreting Covariance\nCovariance measures the direction of the linear relationship between two variables:\n\n\\text{Cov}(Y,Z) &gt; 0: Y and Z tend to move in the same direction (positive relationship)\n\\text{Cov}(Y,Z) &lt; 0: Y and Z tend to move in opposite directions (negative relationship)\n\\text{Cov}(Y,Z) = 0: Y and Z have no linear relationship\n\n\n\n3.2.3 Computing Covariance Using Conditional Expectations\nUsing our previous results for E[YZ], we can express covariance in terms of conditional expectations:\n\\begin{align*}\n\\text{Cov}(Y,Z) &= E[YZ] - E[Y]E[Z] \\\\\n&= E[Z \\cdot E[Y|Z]] - E[Y]E[Z]\n\\end{align*}\nThis formula is particularly useful when we know the conditional expectation E[Y|Z].\n\n\n3.2.4 Properties of Covariance\n\nProperties of Covariance\n\nSymmetry: \\text{Cov}(Y,Z) = \\text{Cov}(Z,Y)\nLinearity in each argument: For constants a, b and random variables Y, Z, W, V: \\begin{align*}\n&\\text{Cov}(aY + Z, bW + V)\\\\\n&= ab\\text{Cov}(Y,W) + a\\text{Cov}(Y,V) + b\\text{Cov}(Z,W) + \\text{Cov}(Z,V)\n\\end{align*}\nVariance as a special case: \\text{Var}(Y) = \\text{Cov}(Y,Y) = E[Y^2] - E[Y]^2\nIndependence implies zero covariance:\nIf Y and Z are independent, then \\text{Cov}(Y,Z) = 0\n\n\n\n\n\n\n3.2.5 Covariance and Independence\nIt’s important to note that while independence implies zero covariance, the converse is not generally true: zero covariance does not imply independence.\nA classic example of variables that have zero covariance yet are dependent is when Y is a standard normal random variable and Z = Y^2. These variables are clearly dependent (knowledge of Y completely determines Z), but:\n\\text{Cov}(Y,Y^2) = E[Y \\cdot Y^2] - E[Y]E[Y^2] = E[Y^3] - E[Y]E[Y^2]\nFor a standard normal variable, E[Y] = 0 and E[Y^3] = 0 (due to symmetry around 0), so: \\text{Cov}(Y,Y^2) = 0 - 0 \\cdot E[Y^2] = 0\nThis highlights that covariance only captures linear relationships between variables, not all forms of dependence.\n\n\n3.2.6 Correlation Coefficient\nWhile covariance measures the direction of association between variables, its magnitude depends on the scales of the variables. The correlation coefficient standardizes this measure to be scale-invariant:\n\nCorrelation Coefficient\nThe correlation coefficient between random variables Y and Z is defined as:\n\\text{Corr}(Y,Z) = \\rho_{Y,Z} = \\frac{\\text{Cov}(Y,Z)}{\\sqrt{\\text{Var}(Y)\\text{Var}(Z)}}\n\n\n\nThe correlation coefficient has the following properties:\n\n-1 \\leq \\rho_{Y,Z} \\leq 1\n\\rho_{Y,Z} = 1 implies a perfect positive linear relationship\n\\rho_{Y,Z} = -1 implies a perfect negative linear relationship\n\\rho_{Y,Z} = 0 implies no linear relationship",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Random Variables</span>"
    ]
  },
  {
    "objectID": "part3_covariance.html#expected-value-vector-and-covariance-matrix",
    "href": "part3_covariance.html#expected-value-vector-and-covariance-matrix",
    "title": "3  Multiple Random Variables",
    "section": "3.3 Expected Value Vector and Covariance Matrix",
    "text": "3.3 Expected Value Vector and Covariance Matrix\nWhen working with multivariate data, we often need to analyze several random variables simultaneously. Let’s consider a k-dimensional random vector \\boldsymbol{Z} = (Z_1, Z_2, \\ldots, Z_k)', where the prime denotes vector transposition.\n\n3.3.1 Expected Value Vector\n\nExpected Value Vector\nThe expected value vector (or mean vector) of a random vector \\boldsymbol{Z} = (Z_1, Z_2, \\ldots, Z_k)' is defined as:\n\\boldsymbol{\\mu}_Z = E[\\boldsymbol{Z}] = (E[Z_1], E[Z_2], \\ldots, E[Z_k])'\n\n\n\nEach component E[Z_i] is calculated according to: E[Z_i] = \\int_{-\\infty}^{\\infty} z_i \\ \\text{d}F_{Z_i}(z_i)\nwhere F_{Z_i} represents the marginal CDF of Z_i.\nThe expected value vector provides the central location of the multivariate distribution, serving as a natural extension of the univariate expected value.\n\n\n3.3.2 Covariance Matrix\n\nCovariance Matrix\nThe covariance matrix of a random vector \\boldsymbol{Z} = (Z_1, Z_2, \\ldots, Z_k)', denoted by \\boldsymbol{\\Sigma}_Z, is defined as:\n\\boldsymbol{\\Sigma}_Z = E[(\\boldsymbol{Z} - \\boldsymbol{\\mu}_Z)(\\boldsymbol{Z} - \\boldsymbol{\\mu}_Z)']\nExpanding this definition, we get a k \\times k matrix:\n\\boldsymbol{\\Sigma}_Z =\n\\begin{pmatrix}\n\\text{Var}(Z_1) & \\text{Cov}(Z_1, Z_2) & \\cdots & \\text{Cov}(Z_1, Z_k) \\\\\n\\text{Cov}(Z_2, Z_1) & \\text{Var}(Z_2) & \\cdots & \\text{Cov}(Z_2, Z_k) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(Z_k, Z_1) & \\text{Cov}(Z_k, Z_2) & \\cdots & \\text{Var}(Z_k)\n\\end{pmatrix}\n\n\n\nIn this matrix:\n\nDiagonal elements \\Sigma_{ii} = \\text{Var}(Z_i) represent the variance of each component\nOff-diagonal elements \\Sigma_{ij} = \\text{Cov}(Z_i, Z_j) represent the covariance between components\n\n\n\n3.3.3 Properties of the Covariance Matrix\n\nProperties of the Covariance Matrix\n\nSymmetry: \\boldsymbol{\\Sigma}_Z = \\boldsymbol{\\Sigma}_Z' since \\text{Cov}(Z_i, Z_j) = \\text{Cov}(Z_j, Z_i)\nPositive Semi-Definiteness: For any non-zero vector \\boldsymbol{a} \\in \\mathbb{R}^k, \\boldsymbol{a}'\\boldsymbol{\\Sigma}_Z\\boldsymbol{a} \\geq 0\nLinear Transformations: For a matrix \\boldsymbol{A} and vector \\boldsymbol{b}, if \\boldsymbol{Y} = \\boldsymbol{A}\\boldsymbol{Z} + \\boldsymbol{b}, then:\n\nE[\\boldsymbol{Y}] = \\boldsymbol{A}E[\\boldsymbol{Z}] + \\boldsymbol{b}\n\\boldsymbol{\\Sigma}_Y = \\boldsymbol{A}\\boldsymbol{\\Sigma}_Z\\boldsymbol{A}'\n\n\n\n\n\nThe positive semi-definiteness of the covariance matrix follows because \\boldsymbol{a}'\\boldsymbol{\\Sigma}_Z\\boldsymbol{a} = \\text{Var}(\\boldsymbol{a}'\\boldsymbol{Z}), which is the variance of a linear combination of the components of \\boldsymbol{Z}, and variance is always non-negative.\n\n\n3.3.4 Correlation Matrix\nThe correlation matrix standardizes the covariance matrix by dividing each covariance by the product of the corresponding standard deviations:\n\nCorrelation Matrix\nThe correlation matrix of a random vector \\boldsymbol{Z}, denoted by \\boldsymbol{R}_Z, is defined as:\n\\boldsymbol{R}_Z =\n\\begin{pmatrix}\n1 & \\rho_{12} & \\cdots & \\rho_{1k} \\\\\n\\rho_{21} & 1 & \\cdots & \\rho_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho_{k1} & \\rho_{k2} & \\cdots & 1\n\\end{pmatrix}\nwhere \\rho_{ij} = \\frac{\\text{Cov}(Z_i, Z_j)}{\\sqrt{\\text{Var}(Z_i)\\text{Var}(Z_j)}} is the correlation coefficient between Z_i and Z_j.\n\n\n\nMathematically, if \\boldsymbol{D} is a diagonal matrix with D_{ii} = \\sqrt{\\text{Var}(Z_i)}, then: \\boldsymbol{R}_Z = \\boldsymbol{D}^{-1}\\boldsymbol{\\Sigma}_Z\\boldsymbol{D}^{-1}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Random Variables</span>"
    ]
  },
  {
    "objectID": "part3_covariance.html#heavy-tails-when-expectations-fail-to-exist",
    "href": "part3_covariance.html#heavy-tails-when-expectations-fail-to-exist",
    "title": "3  Multiple Random Variables",
    "section": "3.4 Heavy Tails: When Expectations Fail to Exist",
    "text": "3.4 Heavy Tails: When Expectations Fail to Exist\nOur previous discussions assumed that expected values and covariance matrices exist, but this isn’t always guaranteed. Some probability distributions have such slow decay in their tails that moments of certain order may be infinite or undefined. These “heavy-tailed” distributions present special challenges for statistical analysis.\n\n3.4.1 Infinite Expectations\n\nInfinite Expectation\nA random variable Y has an infinite expectation if: E[Y] = \\int_{-\\infty}^{\\infty} u \\ \\text{d}F_Y(u) = \\infty\nIn such cases, the sample mean does not converge to any finite value as the sample size increases.\n\n\n\nThe sample mean of i.i.d. samples from most distributions converges to the population mean as sample size increases (a property known as consistency). However, there are exceptional cases where consistency fails because the population mean itself is infinite.\n\n\n3.4.2 Examples of Distributions with Infinite Moments\n\n3.4.2.1 Pareto Distribution\nThe simple Pareto distribution with parameter \\alpha = 1 has the PDF: f(u) = \\begin{cases} \\frac{1}{u^2} & \\text{if} \\ u &gt; 1, \\\\\n0 & \\text{if} \\ u \\leq 1, \\end{cases}\nThe expected value is: E[Y] = \\int_{-\\infty}^\\infty u f(u) \\ \\text{d}u = \\int_{1}^\\infty \\frac{u}{u^2} \\ \\text{d}u = \\int_{1}^\\infty \\frac{1}{u} \\ \\text{d}u = \\log(u)|_1^\\infty = \\infty\nSince the population mean is infinite, the sample mean cannot converge to any finite value and is therefore inconsistent.\n\n\n3.4.2.2 St. Petersburg Paradox\nThe game of chance from the St. Petersburg paradox is a discrete example with infinite expectation. In this game, a fair coin is tossed until a tail appears; if the first tail is on the nth toss, the payoff is 2^n dollars. The expected payoff is: E[Y] = \\sum_{n=1}^{\\infty} 2^n \\cdot \\frac{1}{2^n} = \\sum_{n=1}^{\\infty} 1 = \\infty\nThis infinity arises from the infinite sum of 1’s, reflecting the unbounded potential payoffs in the game.\n\n\n3.4.2.3 Cauchy Distribution\nThe Cauchy distribution (also known as the t-distribution with 1 degree of freedom) has the PDF: f(u) = \\frac{1}{\\pi (1+u^2)}\nThe Cauchy distribution presents a fascinating case where the sample mean of n observations has exactly the same distribution as a single observation, regardless of how large n becomes. This means the sample mean does not converge to any value as sample size increases.\n\n\n3.4.2.4 t-Distribution with Few Degrees of Freedom\n\nCauchy distribution (t_1): No finite moments\nt_2 distribution: Finite mean but infinite variance\nt_3 distribution: Finite variance but infinite skewness\nt_4 distribution: Finite skewness but infinite kurtosis\n\nMore generally, for a t-distribution with m degrees of freedom: E[Y^k] &lt; \\infty \\text{ for } k &lt; m E[Y^k] = \\infty \\text{ for } k \\geq m\n\n\n\n3.4.3 Real-World Examples\nHeavy-tailed distributions arise in many real-world phenomena where extreme events and large outliers are common:\n\nFinancial returns: Stock market crashes and extreme price movements\nIncome and wealth distributions: Extreme wealth concentration\nNatural disasters: Extreme earthquakes, floods, or storms",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Random Variables</span>"
    ]
  }
]